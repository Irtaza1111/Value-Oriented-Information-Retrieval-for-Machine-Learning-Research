from google.colab import drive
drive.mount('/content/drive')

!pip -q install pymupdf pandas openpyxl pdf2image pytesseract


!apt-get -qq update
!apt-get -qq install -y poppler-utils tesseract-ocr



import zipfile
from pathlib import Path
SEED_ZIP = "/content/drive/MyDrive/seed_papers.zip"


SEED_DIR = Path("/content/drive/MyDrive/Seed")

SEED_DIR.mkdir(parents=True, exist_ok=True)


with zipfile.ZipFile(SEED_ZIP, 'r') as zip_ref:
    zip_ref.extractall(SEED_DIR)

print("✅ Seed paper files extracted to:", SEED_DIR)



PDF_DIR = "/content/drive/MyDrive/Seed/seed_papers"  # folder containing PDFs
EXCEL_PATH = "/content/drive/MyDrive/moral_vs_non_moral_grouped_100_papes.xlsx"

MAPPING_OUT = "/content/drive/MyDrive/paperid_to_pdf_mapping_auto.csv"
MAPPING_ONE2ONE_OUT = "/content/drive/MyDrive/paperid_to_pdf_mapping_auto_one2one.csv"


def normalize(s: str) -> str:
    s = (s or "").lower()
    s = s.replace("\u00ad", "")  # soft hyphen
    s = re.sub(r"\s+", " ", s)
    s = re.sub(r"[^a-z0-9 ]+", "", s)
    return s.strip()

def ocr_first_page(pdf_path, dpi=250, lang="eng"):
    images = convert_from_path(pdf_path, dpi=dpi, first_page=1, last_page=1)
    return pytesseract.image_to_string(images[0], lang=lang) if images else ""

def extract_title_from_pdf_first_page(pdf_path):
    """
    Try to extract the title using font-size heuristic:
    - Get spans on page 1
    - Take the largest font size text lines near the top
    Fallback: use first 15 lines of OCR/text if needed.
    """
    try:
        doc = fitz.open(pdf_path)
        page = doc.load_page(0)
        d = page.get_text("dict")
        doc.close()
    except Exception:
        return ""

    spans = []
    for block in d.get("blocks", []):
        for line in block.get("lines", []):
            for span in line.get("spans", []):
                text = span.get("text", "").strip()
                if not text:
                    continue
                size = span.get("size", 0)
                bbox = span.get("bbox", [0,0,0,0])  # [x0,y0,x1,y1]
                y0 = bbox[1]
                spans.append((size, y0, text))

    if not spans:
        return ""

    spans.sort(key=lambda x: x[1])
    top_spans = [s for s in spans if s[1] < 250] 
    if not top_spans:
        top_spans = spans[:80]

   
    max_size = max(s[0] for s in top_spans)
    cand = [t for (sz, y, t) in top_spans if sz >= max_size - 0.5]

    title = " ".join(cand[:30]).strip()
    title = re.sub(r"\s+", " ", title).strip()

    return title[:300]


df = pd.read_excel(EXCEL_PATH)

COL_PAPER_ID = "paperId"
COL_TITLE = "Title"

excel_rows = []
for _, r in df.iterrows():
    pid = str(r[COL_PAPER_ID]).strip()
    title = str(r[COL_TITLE]).strip() if pd.notna(r[COL_TITLE]) else ""
    excel_rows.append({"paperId": pid, "Title": title, "Title_norm": normalize(title)})

excel_df = pd.DataFrame(excel_rows).drop_duplicates(subset=["paperId"], keep="first")
excel_titles = excel_df["Title_norm"].tolist()

# List PDFs
pdf_files = [f for f in os.listdir(PDF_DIR) if f.lower().endswith(".pdf")]
print("PDFs found:", len(pdf_files))
print("Excel papers:", len(excel_df))

results = []

for pdf_fn in tqdm(pdf_files):
    pdf_path = os.path.join(PDF_DIR, pdf_fn)

    # Try title from text
    title_guess = extract_title_from_pdf_first_page(pdf_path)
    title_guess_norm = normalize(title_guess)

    used_ocr = False
    # If empty or too short, OCR fallback
    if len(title_guess_norm) < 10:
        used_ocr = True
        ocr_text = ocr_first_page(pdf_path)
        lines = [ln.strip() for ln in ocr_text.splitlines() if ln.strip()]
        title_guess = " ".join(lines[:3])[:300]
        title_guess_norm = normalize(title_guess)

    scored = []
    for idx, ex_norm in enumerate(excel_titles):
        s = fuzz.token_set_ratio(title_guess_norm, ex_norm)
        scored.append((s, idx))

    scored.sort(reverse=True, key=lambda x: x[0])
    top5 = scored[:5]

    for rank, (score, idx) in enumerate(top5, start=1):
        results.append({
            "pdf_filename": pdf_fn,
            "pdf_title_guess": title_guess,
            "used_ocr": used_ocr,
            "match_rank": rank,
            "match_score": score,
            "paperId": excel_df.iloc[idx]["paperId"],
            "excel_title": excel_df.iloc[idx]["Title"],
        })

map_candidates = pd.DataFrame(results)
map_candidates.to_csv(MAPPING_OUT, index=False)
print("✅ Candidate mapping file saved:", MAPPING_OUT)

display(map_candidates.sort_values("match_score", ascending=False).head(10))


cand = map_candidates.sort_values("match_score", ascending=False).copy()

assigned_papers = set()
assigned_pdfs = set()
final_rows = []

for _, r in cand.iterrows():
    pid = r["paperId"]
    pdf = r["pdf_filename"]
    score = r["match_score"]

    if pid in assigned_papers:
        continue
    if pdf in assigned_pdfs:
        continue

    if score < 75:
        continue

    assigned_papers.add(pid)
    assigned_pdfs.add(pdf)
    final_rows.append({
        "paperId": pid,
        "Title": r["excel_title"],
        "pdf_filename": pdf,
        "match_score": score,
        "pdf_title_guess": r["pdf_title_guess"],
        "used_ocr": r["used_ocr"],
    })

final_map = pd.DataFrame(final_rows)
final_map.to_csv(MAPPING_ONE2ONE_OUT, index=False)
print("✅ One-to-one mapping saved:", MAPPING_ONE2ONE_OUT)
print("Mapped papers:", len(final_map), "out of", len(excel_df))

display(final_map.sort_values("match_score", ascending=True).head(15))


!pip -q install pymupdf pandas openpyxl pdf2image pytesseract
!apt-get -qq update
!apt-get -qq install -y poppler-utils tesseract-ocr

import os, re, json, ast
import pandas as pd
import fitz
from tqdm import tqdm
from pdf2image import convert_from_path
import pytesseract

PDF_DIR = "/content/drive/MyDrive/Seed/seed_papers"
EXCEL_PATH = "/content/drive/MyDrive/moral_vs_non_moral_grouped_100_papes.xlsx"
MAP_ONE2ONE = "/content/drive/MyDrive/paperid_to_pdf_mapping_auto_one2one.csv"
OUTPUT_JSONL = "/content/drive/MyDrive/birhane_94_with_abstracts.jsonl"

MAX_PAGES_FOR_ABSTRACT = 2
OCR_DPI = 300
OCR_LANG = "eng"

COL_PAPER_ID = "paperId"
COL_TITLE = "Title"
COL_AUTHORS = "raw_author_name"
COL_VALUES = "liberal_values"
COL_LABEL_SOURCE = "liberal_categories"


def normalize_authors(authors_cell):
    if authors_cell is None or (isinstance(authors_cell, float) and pd.isna(authors_cell)):
        return []
    s = str(authors_cell).strip()
    if not s:
        return []
    if ";" in s:
        parts = [p.strip() for p in s.split(";")]
    else:
        parts = [p.strip() for p in s.split(",") if p.strip()]
    return [p for p in parts if p]

def normalize_values(values_cell):
    # Handles list, delimiter-separated strings, and stringified lists like "['a','b']"
    if values_cell is None or (isinstance(values_cell, float) and pd.isna(values_cell)):
        return []
    if isinstance(values_cell, list):
        vals = values_cell
    else:
        s = str(values_cell).strip()
        if s.startswith("[") and s.endswith("]"):
            try:
                parsed = ast.literal_eval(s)
                vals = parsed if isinstance(parsed, list) else [s]
            except:
                vals = [s]
        else:
            vals = re.split(r"[;,|/]+", s)
    vals = [str(v).strip().strip("'").strip('"').lower() for v in vals if str(v).strip()]
    seen, out = set(), []
    for v in vals:
        if v not in seen:
            out.append(v); seen.add(v)
    return out

def extract_text_from_pdf(pdf_path, max_pages=2):
    doc = fitz.open(pdf_path)
    text = ""
    for i in range(min(max_pages, len(doc))):
        text += doc.load_page(i).get_text("text") + "\n"
    doc.close()
    return text.strip()

def ocr_first_pages(pdf_path, max_pages=2, dpi=300, lang="eng"):
    images = convert_from_path(pdf_path, dpi=dpi, first_page=1, last_page=max_pages)
    return "\n".join(pytesseract.image_to_string(img, lang=lang) for img in images).strip()

def clean_text(text):
    text = text.replace("\u00ad", "")
    text = re.sub(r"[ \t]{2,}", " ", text)
    text = re.sub(r"\n{3,}", "\n\n", text)
    return text.strip()

def extract_abstract(text):
    if not text:
        return None
    t = clean_text(text)

    stop_headers = r"(?:\n\s*(?:1\s*\.?\s*)?(?:introduction|background|keywords|index terms|related work|preliminaries)\b)"

    m = re.search(
        rf"(?:^|\n)\s*abstract\s*[:\-]?\s*\n?(.*?){stop_headers}",
        t, flags=re.IGNORECASE | re.DOTALL
    )
    if m:
        ab = re.sub(r"\n+", " ", m.group(1)).strip()
        return ab if len(ab) >= 30 else None

    m = re.search(
        rf"(?:^|\n)\s*abstract\s*[—\-:]\s*(.*?){stop_headers}",
        t, flags=re.IGNORECASE | re.DOTALL
    )
    if m:
        ab = re.sub(r"\n+", " ", m.group(1)).strip()
        return ab if len(ab) >= 30 else None

    return None


df = pd.read_excel(EXCEL_PATH).drop_duplicates(subset=[COL_PAPER_ID], keep="first")
mp = pd.read_csv(MAP_ONE2ONE)

df94 = df.merge(mp[["paperId", "pdf_filename"]], left_on=COL_PAPER_ID, right_on="paperId", how="inner")
print("Papers after mapping join:", len(df94))

missing_pdf = 0
abstract_fail = 0
written = 0

with open(OUTPUT_JSONL, "w", encoding="utf-8") as out:
    for _, row in tqdm(df94.iterrows(), total=len(df94)):
        pid = str(row[COL_PAPER_ID]).strip()
        title = str(row[COL_TITLE]).strip() if pd.notna(row[COL_TITLE]) else ""
        authors = normalize_authors(row.get(COL_AUTHORS)) if COL_AUTHORS in df94.columns else []
        values = normalize_values(row.get(COL_VALUES)) if COL_VALUES in df94.columns else []
        labels_source = str(row.get(COL_LABEL_SOURCE, "Birhane_2022")).strip() if COL_LABEL_SOURCE in df94.columns else "Birhane_2022"

        pdf_filename = str(row["pdf_filename"]).strip()
        pdf_path = os.path.join(PDF_DIR, pdf_filename)

        abstract = ""
        used_ocr = False

        if os.path.exists(pdf_path):
            txt = extract_text_from_pdf(pdf_path, max_pages=MAX_PAGES_FOR_ABSTRACT)
            ab = extract_abstract(txt)

            if ab is None:
                used_ocr = True
                ocr_txt = ocr_first_pages(pdf_path, max_pages=MAX_PAGES_FOR_ABSTRACT, dpi=OCR_DPI, lang=OCR_LANG)
                ab = extract_abstract(ocr_txt)

            if ab is None:
                abstract_fail += 1
            else:
                abstract = ab
        else:
            missing_pdf += 1

        rec = {
            "paper_id": pid,
            "title": title,
            "authors": authors,
            "year": None,
            "venue": None,
            "doi": None,
            "url": None,
            "arxiv_id": None,
            "pdf_filename": pdf_filename,
            "abstract": abstract,
            "values": values,
            "labels_source": labels_source,
            "abstract_extraction": {"used_ocr": used_ocr, "pages_checked": MAX_PAGES_FOR_ABSTRACT}
        }

        out.write(json.dumps(rec, ensure_ascii=False) + "\n")
        written += 1

print("\n✅ JSONL saved:", OUTPUT_JSONL)
print("Records written:", written)
print("Missing PDFs (should be 0):", missing_pdf)
print("Abstract failures (abstract=''):", abstract_fail)

# Quick check
import json
with open(OUTPUT_JSONL, "r", encoding="utf-8") as f:
    lines = f.readlines()
print("JSONL lines:", len(lines))
sample = json.loads(lines[0])
print("Sample abstract length:", len(sample["abstract"]))
print("Sample values:", sample["values"][:8])


!pip -q install pandas rank_bm25 nltk

import json
import pandas as pd
import nltk
from nltk.tokenize import word_tokenize

nltk.download("punkt")

JSONL_PATH = "/content/drive/MyDrive/birhane_94_with_abstracts.jsonl"  # <-- change if needed

df = pd.read_json(JSONL_PATH, lines=True)
print("Papers:", len(df))

# Keep only rows with non-empty abstracts (BM25 needs text)
df["abstract"] = df["abstract"].fillna("").astype(str)
df_ok = df[df["abstract"].str.strip() != ""].copy()

print("With non-empty abstracts:", len(df_ok))
df_ok.head(2)


import re

def tokenize(text: str):
    text = text.lower()
    return re.findall(r"\b[a-z0-9]+\b", text)

corpus_tokens = [tokenize(ab) for ab in df_ok["abstract"].tolist()]

from rank_bm25 import BM25Okapi
bm25 = BM25Okapi(corpus_tokens)

print("✅ BM25 rebuilt without NLTK. Docs indexed:", len(corpus_tokens))


def bm25_search(query, top_k=5):
    q_tokens = tokenize(query)
    scores = bm25.get_scores(q_tokens)
    top_idx = scores.argsort()[::-1][:top_k]

    results = []
    for i in top_idx:
        row = df_ok.iloc[i]
        results.append({
            "paper_id": row["paper_id"],
            "title": row["title"],
            "score": float(scores[i]),
            "values": row["values"],
            "snippet": row["abstract"][:250]
        })
    return results

results = bm25_search("fairness accountability machine learning", top_k=5)
for r in results:
    print("----")
    print("score:", round(r["score"], 3))
    print("title:", r["title"])
    print("values:", r["values"])


!pip -q install pandas rank_bm25 numpy

import re, json
import numpy as np
import pandas as pd
from rank_bm25 import BM25Okapi

JSONL_PATH = "/content/drive/MyDrive/birhane_94_with_abstracts.jsonl"  # adjust if needed


df = pd.read_json(JSONL_PATH, lines=True)

df["abstract"] = df["abstract"].fillna("").astype(str)
df = df[df["abstract"].str.strip() != ""].copy()
df = df.reset_index(drop=True)

print("Docs with abstracts:", len(df))

def tokenize(text: str):
    return re.findall(r"\b[a-z0-9]+\b", text.lower())

corpus_tokens = [tokenize(ab) for ab in df["abstract"].tolist()]
bm25 = BM25Okapi(corpus_tokens)


def bm25_retrieve(query: str, top_k: int = 100):
    q_tokens = tokenize(query)
    scores = bm25.get_scores(q_tokens)
    ranked_idx = np.argsort(scores)[::-1]
    ranked_idx = ranked_idx[:min(top_k, len(ranked_idx))]
    return ranked_idx, scores

def is_relevant(paper_values, target_values):
    pv = set([str(v).strip().lower() for v in (paper_values or [])])
    tv = set([str(v).strip().lower() for v in (target_values or [])])
    return len(pv.intersection(tv)) > 0


def precision_at_k(rels, k):
    rels_k = rels[:k]
    return sum(rels_k) / k

def recall_at_k(rels, total_relevant, k):
    if total_relevant == 0:
        return np.nan
    return sum(rels[:k]) / total_relevant

def mrr_at_k(rels, k):
    for i, r in enumerate(rels[:k], start=1):
        if r:
            return 1.0 / i
    return 0.0

def dcg_at_k(rels, k):
    dcg = 0.0
    for i, r in enumerate(rels[:k], start=1):
        if r:
            dcg += 1.0 / np.log2(i + 1)
    return dcg

def ndcg_at_k(rels, total_relevant, k):
    if total_relevant == 0:
        return np.nan
    dcg = dcg_at_k(rels, k)
    ideal_rels = [1] * min(total_relevant, k)
    idcg = dcg_at_k(ideal_rels, k)
    return dcg / idcg if idcg > 0 else np.nan


QUERY_SET = [
    {"qid": "Q01", "query": "fairness in machine learning", "target_values": ["fairness"]},
    {"qid": "Q02", "query": "privacy preserving machine learning", "target_values": ["privacy"]},
    {"qid": "Q03", "query": "robustness to adversarial attacks", "target_values": ["robustness", "security"]},
    {"qid": "Q04", "query": "efficient scalable deep learning", "target_values": ["efficiency", "scales up", "large scale", "parallelizability / distributed"]},
    {"qid": "Q05", "query": "interpretability and transparency of models", "target_values": ["transparency", "interpretability", "explainability"]},
    {"qid": "Q06", "query": "scientific methodology and experiments", "target_values": ["scientific methodology", "quantitative evidence (e.g. experiments)"]},
    {"qid": "Q07", "query": "generalization guarantees and theory", "target_values": ["generalization", "theoretical guarantees", "formal description/analysis"]},
    {"qid": "Q08", "query": "beneficence and social good in AI", "target_values": ["beneficence", "social good"]},
]

rows = []

for q in QUERY_SET:
    qid = q["qid"]
    query = q["query"]
    target_values = q["target_values"]

    total_rel = 0
    for vals in df["values"].tolist():
        if is_relevant(vals, target_values):
            total_rel += 1

    ranked_idx, scores = bm25_retrieve(query, top_k=100)

    rels = []
    for i in ranked_idx:
        rels.append(1 if is_relevant(df.loc[i, "values"], target_values) else 0)

    p10 = precision_at_k(rels, 10)
    r100 = recall_at_k(rels, total_rel, 100)
    mrr10 = mrr_at_k(rels, 10)
    ndcg10 = ndcg_at_k(rels, total_rel, 10)

    rows.append({
        "qid": qid,
        "query": query,
        "target_values": ", ".join(target_values),
        "total_relevant_in_corpus": total_rel,
        "P@10": p10,
        "MRR@10": mrr10,
        "nDCG@10": ndcg10,
        "Recall@100": r100
    })

results = pd.DataFrame(rows)

print("\n=== Per-query results ===")
display(results)

print("\n=== Averages (macro-average over queries) ===")
avg = results[["P@10", "MRR@10", "nDCG@10", "Recall@100"]].mean(numeric_only=True)
print(avg)


!pip -q install sentence-transformers faiss-cpu pandas numpy

import re
import numpy as np
import pandas as pd
from sentence_transformers import SentenceTransformer

JSONL_PATH = "/content/drive/MyDrive/birhane_94_with_abstracts.jsonl"  # adjust if needed

df = pd.read_json(JSONL_PATH, lines=True)
df["abstract"] = df["abstract"].fillna("").astype(str)
df = df[df["abstract"].str.strip() != ""].reset_index(drop=True)

print("Docs with abstracts:", len(df))


import faiss

MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"

model = SentenceTransformer(MODEL_NAME)

emb = model.encode(
    df["abstract"].tolist(),
    batch_size=32,
    show_progress_bar=True,
    normalize_embeddings=True
).astype("float32")

dim = emb.shape[1]
index = faiss.IndexFlatIP(dim)   # inner product; with normalized vectors this = cosine similarity
index.add(emb)

print("✅ FAISS index built. Vectors:", index.ntotal, "Dim:", dim)


def dense_retrieve(query: str, top_k: int = 100):
    q = model.encode([query], normalize_embeddings=True).astype("float32")
    scores, idx = index.search(q, top_k)  # scores shape (1, top_k)
    return idx[0], scores[0]


def is_relevant(paper_values, target_values):
    pv = set([str(v).strip().lower() for v in (paper_values or [])])
    tv = set([str(v).strip().lower() for v in (target_values or [])])
    return len(pv.intersection(tv)) > 0

def precision_at_k(rels, k):
    return sum(rels[:k]) / k

def recall_at_k(rels, total_relevant, k):
    if total_relevant == 0:
        return np.nan
    return sum(rels[:k]) / total_relevant

def mrr_at_k(rels, k):
    for i, r in enumerate(rels[:k], start=1):
        if r:
            return 1.0 / i
    return 0.0

def dcg_at_k(rels, k):
    dcg = 0.0
    for i, r in enumerate(rels[:k], start=1):
        if r:
            dcg += 1.0 / np.log2(i + 1)
    return dcg

def ndcg_at_k(rels, total_relevant, k):
    if total_relevant == 0:
        return np.nan
    dcg = dcg_at_k(rels, k)
    ideal_rels = [1] * min(total_relevant, k)
    idcg = dcg_at_k(ideal_rels, k)
    return dcg / idcg if idcg > 0 else np.nan


def dense_retrieve(query: str, top_k: int = 100):
    top_k = min(top_k, len(df))  # never ask for more than we have
    q = model.encode([query], normalize_embeddings=True).astype("float32")
    scores, idx = index.search(q, top_k)
    idx = idx[0].tolist()
    scores = scores[0].tolist()

    cleaned = [(i, s) for i, s in zip(idx, scores) if i != -1]
    idx_clean = [i for i, _ in cleaned]
    scores_clean = [s for _, s in cleaned]
    return idx_clean, scores_clean


rows = []
for q in QUERY_SET:
    qid, query, target_values = q["qid"], q["query"], q["target_values"]

    total_rel = sum(is_relevant(vals, target_values) for vals in df["values"].tolist())

    idx, scores = dense_retrieve(query, top_k=100)

    rels = []
    for i in idx:
        rels.append(1 if is_relevant(df.iloc[i]["values"], target_values) else 0)

    rows.append({
        "qid": qid,
        "query": query,
        "target_values": ", ".join(target_values),
        "total_relevant_in_corpus": total_rel,
        "P@10": precision_at_k(rels, 10),
        "MRR@10": mrr_at_k(rels, 10),
        "nDCG@10": ndcg_at_k(rels, total_rel, 10),
        "Recall@100": recall_at_k(rels, total_rel, 100)
    })

dense_results = pd.DataFrame(rows)
print("\n=== Dense (SBERT+FAISS) per-query results ===")
display(dense_results)

print("\n=== Dense averages (macro) ===")
print(dense_results[["P@10","MRR@10","nDCG@10","Recall@100"]].mean(numeric_only=True))


bm25_results = results.copy()        # from BM25 evaluation


dense_results = dense_results.copy() # from SBERT evaluation


import matplotlib.pyplot as plt
import numpy as np

metrics = ["P@10", "MRR@10", "nDCG@10", "Recall@100"]

bm25_avg = bm25_results[metrics].mean()
dense_avg = dense_results[metrics].mean()

x = np.arange(len(metrics))
width = 0.35

plt.figure(figsize=(8,5))
plt.bar(x - width/2, bm25_avg, width, label="BM25")
plt.bar(x + width/2, dense_avg, width, label="SBERT + FAISS")

plt.xticks(x, metrics)
plt.ylabel("Score")
plt.title("BM25 vs Dense Retrieval (Macro Average)")
plt.legend()
plt.grid(axis="y", alpha=0.3)

plt.tight_layout()
plt.show()


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

bm25_results["qid"] = bm25_results["qid"].astype(str)
dense_results["qid"] = dense_results["qid"].astype(str)

aligned = bm25_results[["qid", "P@10"]].merge(
    dense_results[["qid", "P@10"]],
    on="qid",
    how="inner",
    suffixes=("_BM25", "_SBERT")
).sort_values("qid")

print("BM25 queries:", len(bm25_results), "Dense queries:", len(dense_results), "Aligned:", len(aligned))
display(aligned)

# Plot
x = np.arange(len(aligned))

plt.figure(figsize=(10,5))
plt.bar(x - 0.2, aligned["P@10_BM25"], 0.4, label="BM25")
plt.bar(x + 0.2, aligned["P@10_SBERT"], 0.4, label="SBERT + FAISS")

plt.xticks(x, aligned["qid"])
plt.xlabel("Query ID")
plt.ylabel("P@10")
plt.title("Per-query Precision@10 (Aligned Queries)")
plt.legend()
plt.grid(axis="y", alpha=0.3)
plt.tight_layout()
plt.show()


narrow_qids = ["Q01", "Q02", "Q08"]  # fairness, privacy, beneficence
broad_qids  = ["Q03", "Q04", "Q06", "Q07"]

def group_avg(df, qids):
    return df[df["qid"].isin(qids)]["P@10"].mean()

bm25_narrow = group_avg(bm25_results, narrow_qids)
dense_narrow = group_avg(dense_results, narrow_qids)

bm25_broad = group_avg(bm25_results, broad_qids)
dense_broad = group_avg(dense_results, broad_qids)


labels = ["Narrow (Ethical)", "Broad (Technical)"]
bm25_vals = [bm25_narrow, bm25_broad]
dense_vals = [dense_narrow, dense_broad]

x = np.arange(len(labels))

plt.figure(figsize=(6,5))
plt.bar(x - 0.2, bm25_vals, 0.4, label="BM25")
plt.bar(x + 0.2, dense_vals, 0.4, label="SBERT")

plt.xticks(x, labels)
plt.ylabel("Average P@10")
plt.title("Performance by Query Type")
plt.legend()
plt.grid(axis="y", alpha=0.3)

plt.tight_layout()
plt.show()
