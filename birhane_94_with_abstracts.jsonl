{"paper_id": "00bbfde6af97ce5efcf86b3401d265d42a95603d", "title": "Feature hashing for large scale multitask learning", "authors": ["Kilian Weinberger", "Anirban Dasgupta", "John Langford", "Alex Smola", "Josh Attenberg"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "1553374.1553516.pdf", "abstract": "Empirical evidence suggests that hashing is an effective strategy for dimensionality reduction and practical nonparametric estimation. In this paper we provide exponential tail bounds for fea- ture hashing and show that the interaction be- tween random subspaces is negligible with high probability. We demonstrate the feasibility of this approach with experimental results for a new use case — multitask learning with hundreds of thousands of tasks.", "values": ["novelty", "generalization", "quantitative evidence (e.g. experiments)", "scientific methodology", "large scale", "performance", "efficiency", "effectiveness", "understanding (for researchers)", "applies to real world", "practical", "building on past work"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "0228810a988f6b8f06337e14f564e2fd3f6e1056", "title": "The Recurrent Temporal Restricted Boltzmann Machine", "authors": ["Ilya Sutskever", "Geoffrey E. Hinton", "Graham W. Taylor"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "NIPS-2008-the-recurrent-temporal-restricted-boltzmann-machine-Paper.pdf", "abstract": "The Temporal Restricted Boltzmann Machine (TRBM) is a probabilistic model for sequences that is able to successfully model (i.e., generate nice-looking samples of) several very high dimensional sequences, such as motion capture data and the pixels of low resolution videos of balls bouncing in a box. The major disadvan- tage of the TRBM is that exact inference is extremely hard, since even computing a Gibbs update for a single variable of the posterior is exponentially expensive. This difﬁculty has necessitated the use of a heuristic inference procedure, that nonetheless was accurate enough for successful learning. In this paper we intro- duce the Recurrent TRBM, which is a very slight modiﬁcation of the TRBM for which exact inference is very easy and exact gradient learning is almost tractable. We demonstrate that the RTRBM is better than an analogous TRBM at generating motion capture and videos of bouncing balls.", "values": ["generalization", "realistic output", "performance", "successful", "useful"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "02c388d43f619146ef64babb4c848190e83add1b", "title": "Group lasso with overlap and graph lasso", "authors": ["Laurent Jacob", "Guillaume Obozinski", "Jean-Philippe Vert"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "1553374.1553431.pdf", "abstract": "We propose a new penalty function which, when used as regularization for empirical risk mini- mization procedures, leads to sparse estimators. The support of the sparse vector is typically a union of potentially overlapping groups of co- variates deﬁned a priori, or a set of covariates which tend to be connected to each other when a graph of covariates is given. We study theo- retical properties of the estimator, and illustrate its behavior on simulated and breast cancer gene expression data.", "values": ["novelty", "generalization", "formal description/analysis", "scientific methodology", "promising", "generality", "successful", "used in practice/popular", "scales up", "applies to real world", "interpretable (to users)", "building on past work"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "0389a414c5d0ef50e06fe0c15f6102f374ce1b04", "title": "A dual coordinate descent method for large-scale linear SVM", "authors": ["Cho-Jui Hsieh", "Kai-Wei Chang", "Chih-Jen Lin", "S. Sathiya Keerthi", "S. Sundararajan"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "1390156.1390208.pdf", "abstract": "In many applications, data appear with a huge number of instances as well as features. Linear Support Vector Machines (SVM) is one of the most popular tools to deal with such large-scale sparse data. This paper presents a novel dual coordinate descent method for linear SVM with L1- and L2- loss functions. The proposed method is sim- ple and reaches an ϵ-accurate solution in O(log(1/ϵ)) iterations. Experiments indicate that our method is much faster than state of the art solvers such as Pegasos, TRON, SVMperf, and a recent primal coordinate de- scent implementation.", "values": ["novelty", "simplicity", "generalization", "formal description/analysis", "theoretical guarantees", "quantitative evidence (e.g. experiments)", "large scale", "performance", "efficiency", "easy to implement", "scales up", "applies to real world", "building on past work"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "03e7e8663c69e691be6b6403b1eb1bbf593d31f2", "title": "Gradient Descent Finds Global Minima of Deep Neural Networks", "authors": ["Simon S. Du", "Jason D. Lee", "Haochuan Li", "Liwei Wang", "Xiyu Zhai"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "du19c.pdf", "abstract": "Gradient descent ﬁnds a global minimum in training deep neural networks despite the objec- tive function being non-convex. The current pa- per proves gradient descent achieves zero train- ing loss in polynomial time for a deep over- parameterized neural network with residual con- nections (ResNet). Our analysis relies on the par- ticular structure of the Gram matrix induced by the neural network architecture. This structure al- lows us to show the Gram matrix is stable through- out the training process and this stability implies the global optimality of the gradient descent al- gorithm. We further extend our analysis to deep residual convolutional neural networks and obtain a similar convergence result.", "values": ["novelty", "generalization", "formal description/analysis", "theoretical guarantees", "performance", "efficiency", "identifying limitations", "understanding (for researchers)", "used in practice/popular", "building on past work"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "04541599accc47d8174f63345ce9c987ef21685b", "title": "Disentangling by Factorising", "authors": ["Hyunjik Kim", "Andriy Mnih"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "kim18b.pdf", "abstract": "We deﬁne and address the problem of unsuper- vised learning of disentangled representations on data generated from independent factors of varia- tion. We propose FactorVAE, a method that dis- entangles by encouraging the distribution of rep- resentations to be factorial and hence independent across the dimensions. We show that it improves upon β-VAE by providing a better trade-off be- tween disentanglement and reconstruction quality. Moreover, we highlight the problems of a com- monly used disentanglement metric and introduce a new metric that does not suffer from them.", "values": ["novelty", "simplicity", "formal description/analysis", "quantitative evidence (e.g. experiments)", "qualitative evidence (e.g. examples)", "scientific methodology", "human-like mechanism", "performance", "efficiency", "effectiveness", "identifying limitations", "understanding (for researchers)", "improvement", "used in practice/popular", "useful", "interpretable (to users)"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "0728914a1dba0417bf2847548aa15711f3f8d4e8", "title": "Multi-view clustering via canonical correlation analysis", "authors": ["Kamalika Chaudhuri", "Sham M. Kakade", "Karen Livescu", "Karthik Sridharan"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "1553374.1553391.pdf", "abstract": "Clustering data in high dimensions is be- lieved to be a hard problem in general. A number of eﬃcient clustering algorithms de- veloped in recent years address this prob- lem by projecting the data into a lower- dimensional subspace, e.g. via Principal Components Analysis (PCA) or random pro- jections, before clustering. Here, we consider constructing such projections using multiple views of the data, via Canonical Correlation Analysis (CCA). Under the assumption that the views are un- correlated given the cluster label, we show that the separation conditions required for the algorithm to be successful are signiﬁ- cantly weaker than prior results in the lit- erature. We provide results for mixtures of Gaussians and mixtures of log concave distributions. We also provide empirical support from audio-visual speaker clustering (where we desire the clusters to correspond to speaker ID) and from hierarchical Wikipedia document clustering (where one view is the words in the document and the other is the link structure).", "values": ["novelty", "simplicity", "generalization", "robustness", "formal description/analysis", "theoretical guarantees", "approximation", "quantitative evidence (e.g. experiments)", "qualitative evidence (e.g. examples)", "large scale", "generality", "performance", "efficiency", "successful", "understanding (for researchers)", "improvement", "parallelizability / distributed", "scales up", "applies to real world", "explicability", "easy to work with", "building on past work"], "labels_source": "Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "08d0ea90b53aba0008d25811268fe46562cfb38c", "title": "On the quantitative analysis of deep belief networks", "authors": ["Ruslan Salakhutdinov", "Iain Murray"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "1390156.1390266.pdf", "abstract": "Deep Belief Networks (DBN’s) are generative models that contain many layers of hidden vari- ables. Efﬁcient greedy algorithms for learning and approximate inference have allowed these models to be applied successfully in many ap- plication domains. The main building block of a DBN is a bipartite undirected graphical model called a restricted Boltzmann machine (RBM). Due to the presence of the partition function, model selection, complexity control, and exact maximum likelihood learning in RBM’s are in- tractable. We show that Annealed Importance Sampling (AIS) can be used to efﬁciently es- timate the partition function of an RBM, and we present a novel AIS scheme for comparing RBM’s with different architectures. We further show how an AIS estimator, along with approx- imate inference, can be used to estimate a lower bound on the log-probability that a DBN model with multiple hidden layers assigns to the test data. This is, to our knowledge, the ﬁrst step towards obtaining quantitative results that would allow us to directly assess the performance of Deep Belief Networks as generative models of data.", "values": ["novelty", "generalization", "quantitative evidence (e.g. experiments)", "performance", "efficiency", "successful", "applies to real world"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "1029daa28aa772e441470e61bdd610c222e92932", "title": "On Exact Computation with an Infinitely Wide Neural Net", "authors": ["Sanjeev Arora", "Simon S. Du", "Wei Hu", "Zhiyuan Li", "Ruslan Salakhutdinov", "Ruosong Wang"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "NeurIPS-2019-on-exact-computation-with-an-infinitely-wide-neural-net-Paper.pdf", "abstract": "How well does a classic deep net architecture like AlexNet or VGG19 classify on a standard dataset such as CIFAR-10 when its “width”— namely, number of channels in convolutional layers, and number of nodes in fully-connected internal layers — is allowed to increase to inﬁnity? Such questions have come to the forefront in the quest to theoretically understand deep learning and its mysteries about optimization and generalization. They also connect deep learning to notions such as Gaussian processes and kernels. A recent paper [Jacot et al., 2018] introduced the Neural Tangent Kernel (NTK) which captures the behavior of fully-connected deep nets in the inﬁnite width limit trained by gradient descent; this object was implicit in some other recent papers. An attraction of such ideas is that a pure kernel-based method is used to capture the power of a fully-trained deep net of inﬁnite width. The current paper gives the ﬁrst efﬁcient exact algorithm for computing the ex- tension of NTK to convolutional neural nets, which we call Convolutional NTK (CNTK), as well as an efﬁcient GPU implementation of this algorithm. This results in a signiﬁcant new benchmark for performance of a pure kernel-based method on CIFAR-10, being 10% higher than the methods reported in [Novak et al., 2019], and only 6% lower than the performance of the corresponding ﬁnite deep net architecture (once batch normalization etc. are turned off). Theoretically, we also give the ﬁrst non-asymptotic proof showing that a fully-trained sufﬁciently wide net is indeed equivalent to the kernel regression predictor using NTK.", "values": ["novelty", "simplicity", "generalization", "formal description/analysis", "theoretical guarantees", "approximation", "quantitative evidence (e.g. experiments)", "exactness", "performance", "efficiency", "unifying ideas or integrating components", "understanding (for researchers)", "parallelizability / distributed", "practical", "building on past work"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "14558cb69319eed0d5bfc5648aafcd09d882f443", "title": "Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks", "authors": ["Sanjeev Arora", "Simon S. Du", "Wei Hu", "Zhiyuan Li", "Ruosong Wang"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "arora19a.pdf", "abstract": "Recent works have cast some light on the mys- tery of why deep nets ﬁt any data and generalize despite being very overparametrized. This paper analyzes training and generalization for a simple 2-layer ReLU net with random initialization, and provides the following improvements over recent works: (i) Using a tighter characterization of training speed than recent papers, an explanation for why training a neural net with random la- bels leads to slower training, as originally observed in [Zhang et al. ICLR’17]. (ii) Generalization bound independent of net- work size, using a data-dependent complex- ity measure. Our measure distinguishes clearly between random labels and true la- bels on MNIST and CIFAR, as shown by experiments. Moreover, recent papers re- quire sample complexity to increase (slowly) with the size, while our sample complexity is completely independent of the network size. (iii) Learnability of a broad class of smooth func- tions by 2-layer ReLU nets trained via gradi- ent descent. The key idea is to track dynamics of training and generalization via properties of a related kernel.", "values": ["simplicity", "generalization", "formal description/analysis", "theoretical guarantees", "quantitative evidence (e.g. experiments)", "performance", "efficiency", "understanding (for researchers)", "scales up", "applies to real world", "building on past work"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "1c71771c701aadfd72c5866170a9f5d71464bb88", "title": "Unified Language Model Pre-training for Natural Language Understanding and Generation", "authors": ["Li Dong", "Nan Yang", "Wenhui Wang", "Furu Wei", "Xiaodong Liu", "Yu Wang", "Jianfeng Gao", "Ming Zhou", "Hsiao-Wuen Hon"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "NeurIPS-2019-unified-language-model-pre-training-for-natural-language-understanding-and-generation-Paper.pdf", "abstract": "This paper presents a new UNIﬁed pre-trained Language Model (UNILM) that can be ﬁne-tuned for both natural language understanding and generation tasks. The model is pre-trained using three types of language modeling tasks: unidirec- tional, bidirectional, and sequence-to-sequence prediction. The uniﬁed modeling is achieved by employing a shared Transformer network and utilizing speciﬁc self-attention masks to control what context the prediction conditions on. UNILM compares favorably with BERT on the GLUE benchmark, and the SQuAD 2.0 and CoQA question answering tasks. Moreover, UNILM achieves new state-of- the-art results on ﬁve natural language generation datasets, including improving the CNN/DailyMail abstractive summarization ROUGE-L to 40.51 (2.04 absolute improvement), the Gigaword abstractive summarization ROUGE-L to 35.75 (0.86 absolute improvement), the CoQA generative question answering F1 score to 82.5 (37.1 absolute improvement), the SQuAD question generation BLEU-4 to 22.12 (3.75 absolute improvement), and the DSTC7 document-grounded dialog response generation NIST-4 to 2.67 (human performance is 2.65). The code and pre-trained models are available at https://github.com/microsoft/unilm.", "values": ["novelty", "generalization", "quantitative evidence (e.g. experiments)", "qualitative evidence (e.g. examples)", "large scale", "generality", "performance", "efficiency", "effectiveness", "improvement", "reproducibility", "facilitating use (e.g. sharing code)", "applies to real world", "practical", "building on past work"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "1e80f755bcbf10479afd2338cec05211fdbd325c", "title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations", "authors": ["Honglak Lee", "Roger Grosse", "Rajesh Ranganath", "Andrew Y. Ng"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "1553374.1553453.pdf", "abstract": "There has been much interest in unsuper- vised learning of hierarchical generative mod- els such as deep belief networks. Scaling such models to full-sized, high-dimensional images remains a diﬃcult problem. To ad- dress this problem, we present the convolu- tional deep belief network, a hierarchical gen- erative model which scales to realistic image sizes. This model is translation-invariant and supports eﬃcient bottom-up and top-down probabilistic inference. Key to our approach is probabilistic max-pooling, a novel technique which shrinks the representations of higher layers in a probabilistically sound way. Our experiments show that the algorithm learns useful high-level visual features, such as ob- ject parts, from unlabeled images of objects and natural scenes. We demonstrate excel- lent performance on several visual recogni- tion tasks and show that our model can per- form hierarchical (bottom-up and top-down) inference over full-sized images.", "values": ["novelty", "simplicity", "generalization", "realistic output", "approximation", "quantitative evidence (e.g. experiments)", "qualitative evidence (e.g. examples)", "human-like mechanism", "large scale", "promising", "performance", "efficiency", "successful", "understanding (for researchers)", "scales up", "useful"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "1e826f01d02a8d514b8a687932d228781243496e", "title": "An accelerated gradient method for trace norm minimization", "authors": ["Shuiwang Ji", "Jieping Ye"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "1553374.1553434.pdf", "abstract": "We consider the minimization of a smooth loss function regularized by the trace norm of the matrix variable. Such formulation ﬁnds applications in many machine learning tasks including multi-task learning, matrix classi- ﬁcation, and matrix completion. The stan- dard semideﬁnite programming formulation for this problem is computationally expen- sive. In addition, due to the non-smooth na- ture of the trace norm, the optimal ﬁrst-order black-box method for solving such class of problems converges as O( 1 √ k), where k is the iteration counter. In this paper, we exploit the special structure of the trace norm, based on which we propose an extended gradient al- gorithm that converges as O( 1 k). We further propose an accelerated gradient algorithm, which achieves the optimal convergence rate of O( 1 k2 ) for smooth problems. Experiments on multi-task learning problems demonstrate the eﬃciency of the proposed algorithms.", "values": ["novelty", "simplicity", "generalization", "formal description/analysis", "theoretical guarantees", "approximation", "quantitative evidence (e.g. experiments)", "qualitative evidence (e.g. examples)", "large scale", "generality", "performance", "efficiency", "effectiveness", "unifying ideas or integrating components", "identifying limitations", "improvement", "used in practice/popular", "requires few resources", "applies to real world", "practical", "building on past work"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "1f4294d8e0b0c8559479fac569fc0ea91b4dc0bd", "title": "Adversarial Examples Are Not Bugs, They Are Features", "authors": ["Andrew Ilyas", "Shibani Santurkar", "Dimitris Tsipras", "Logan Engstrom", "Brandon Tran", "Aleksander Madry"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "NeurIPS-2019-adversarial-examples-are-not-bugs-they-are-features-Paper.pdf", "abstract": "Adversarial examples have attracted signiﬁcant attention in machine learning, but the reasons for their existence and pervasiveness remain unclear. We demonstrate that adversarial examples can be directly attributed to the presence of non-robust features: features (derived from patterns in the data distribution) that are highly predictive, yet brittle and (thus) incomprehensible to humans. After capturing these features within a theoretical framework, we establish their widespread ex- istence in standard datasets. Finally, we present a simple setting where we can rigorously tie the phenomena we observe in practice to a misalignment between the (human-speciﬁed) notion of robustness and the inherent geometry of the data.", "values": ["novelty", "generalization", "robustness", "formal description/analysis", "theoretical guarantees", "quantitative evidence (e.g. experiments)", "scientific methodology", "preciseness", "concreteness", "performance", "identifying limitations", "critique", "understanding (for researchers)", "applies to real world", "learning from humans", "interpretable (to users)", "building on past work"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "21b786b3f870fc7fa247c143aa41de88b1fc6141", "title": "Glow: Generative Flow with Invertible 1x1 Convolutions", "authors": ["Diederik P. Kingma", "Diederik P. Kingma", "Prafulla Dhariwal"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "NeurIPS-2018-glow-generative-flow-with-invertible-1x1-convolutions-Paper.pdf", "abstract": "Flow-based generative models (Dinh et al., 2014) are conceptually attractive due to tractability of the exact log-likelihood, tractability of exact latent-variable inference, and parallelizability of both training and synthesis. In this paper we propose Glow, a simple type of generative ﬂow using an invertible 1 × 1 convolution. Using our method we demonstrate a signiﬁcant improvement in log-likelihood on standard benchmarks. Perhaps most strikingly, we demonstrate that a ﬂow-based generative model optimized towards the plain log-likelihood objective is capable of efﬁcient realistic-looking synthesis and manipulation of large images. The code for our model is available at https://github.com/openai/glow.", "values": ["novelty", "simplicity", "generalization", "robustness", "realistic output", "approximation", "quantitative evidence (e.g. experiments)", "qualitative evidence (e.g. examples)", "scientific methodology", "human-like mechanism", "large scale", "promising", "generality", "exactness", "performance", "efficiency", "identifying limitations", "improvement", "used in practice/popular", "parallelizability / distributed", "facilitating use (e.g. sharing code)", "learning from humans", "useful", "easy to work with", "realistic world model", "building on past work"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "2eacb358ba2c06a8706a58ed60c3a6a06d38fec0", "title": "Local Gaussian Process Regression for Real Time Online Model Learning", "authors": ["Duy Nguyen-Tuong", "Jan Peters", "Matthias Seeger"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "NIPS-2008-local-gaussian-process-regression-for-real-time-online-model-learning-Paper.pdf", "abstract": "Learning in real-time applications, e.g., online approximation of the inverse dy- namics model for model-based robot control, requires fast online regression tech- niques. Inspired by local learning, we propose a method to speed up standard Gaussian process regression (GPR) with local GP models (LGP). The training data is partitioned in local regions, for each an individual GP model is trained. The prediction for a query point is performed by weighted estimation using nearby local models. Unlike other GP approximations, such as mixtures of experts, we use a distance based measure for partitioning of the data and weighted prediction. The proposed method achieves online learning and prediction in real-time. Com- parisons with other non-parametric regression methods show that LGP has higher accuracy than LWPR and close to the performance of standard GPR and ν-SVR.", "values": ["novelty", "simplicity", "generalization", "approximation", "quantitative evidence (e.g. experiments)", "controllability (of model owner)", "large scale", "preciseness", "performance", "efficiency", "unifying ideas or integrating components", "identifying limitations", "scales up", "applies to real world", "easy to work with", "realistic world model", "building on past work"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "2f041bde7e1968427b09ce428116b21152c7e715", "title": "Evaluation methods for topic models", "authors": ["Hanna M. Wallach", "Iain Murray", "Ruslan Salakhutdinov", "David Mimno"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "1553374.1553515.pdf", "abstract": "A natural evaluation metric for statistical topic models is the probability of held-out documents given a trained model. While exact computation of this probability is in- tractable, several estimators for this prob- ability have been used in the topic model- ing literature, including the harmonic mean method and empirical likelihood method. In this paper, we demonstrate experimentally that commonly-used methods are unlikely to accurately estimate the probability of held- out documents, and propose two alternative methods that are both accurate and eﬃcient.", "values": ["novelty", "simplicity", "generalization", "approximation", "quantitative evidence (e.g. experiments)", "scientific methodology", "large scale", "generality", "exactness", "performance", "efficiency", "understanding (for researchers)", "used in practice/popular", "applies to real world", "useful", "interpretable (to users)", "building on past work"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "3137bc367c61c0e507a5e3c1f8caeb26f292d79f", "title": "Measuring Invariances in Deep Networks", "authors": ["Ian Goodfellow", "Honglak Lee", "Quoc V. Le", "Andrew Saxe", "Andrew Y. Ng"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "NIPS-2009-measuring-invariances-in-deep-networks-Paper.pdf", "abstract": "For many pattern recognition tasks, the ideal input feature would be invariant to multiple confounding properties (such as illumination and viewing angle, in com- puter vision applications). Recently, deep architectures trained in an unsupervised manner have been proposed as an automatic method for extracting useful features. However, it is difﬁcult to evaluate the learned features by any means other than using them in a classiﬁer. In this paper, we propose a number of empirical tests that directly measure the degree to which these learned features are invariant to different input transformations. We ﬁnd that stacked autoencoders learn modestly increasingly invariant features with depth when trained on natural images. We ﬁnd that convolutional deep belief networks learn substantially more invariant features in each layer. These results further justify the use of “deep” vs. “shallower” repre- sentations, but suggest that mechanisms beyond merely stacking one autoencoder on top of another may be important for achieving invariance. Our evaluation met- rics can also be used to evaluate future work in deep learning, and thus help the development of future algorithms.", "values": ["generalization", "robustness", "theoretical guarantees", "quantitative evidence (e.g. experiments)", "qualitative evidence (e.g. examples)", "promising", "automatic", "performance", "efficiency", "identifying limitations", "understanding (for researchers)", "useful"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "3263ff3c16220322a3989d4f8bde16a53d9b8d45", "title": "Online Metric Learning and Fast Similarity Search", "authors": ["Prateek Jain", "Brian Kulis", "Inderjit S. Dhillon", "Kristen Grauman"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "NIPS-2008-online-metric-learning-and-fast-similarity-search-Paper.pdf", "abstract": "Metric learning algorithms can provide useful distance functions for a variety of domains, and recent work has shown good accuracy for problems where the learner can access all distance constraints at once. However, in many real appli- cations, constraints are only available incrementally, thus necessitating methods that can perform online updates to the learned metric. Existing online algorithms offer bounds on worst-case performance, but typically do not perform well in practice as compared to their ofﬂine counterparts. We present a new online metric learning algorithm that updates a learned Mahalanobis metric based on LogDet regularization and gradient descent. We prove theoretical worst-case performance bounds, and empirically compare the proposed method against existing online metric learning algorithms. To further boost the practicality of our approach, we develop an online locality-sensitive hashing scheme which leads to efﬁcient up- dates to data structures used for fast approximate similarity search. We demon- strate our algorithm on multiple datasets and show that it outperforms relevant baselines.", "values": ["novelty", "simplicity", "generalization", "formal description/analysis", "theoretical guarantees", "approximation", "quantitative evidence (e.g. experiments)", "large scale", "automatic", "performance", "efficiency", "effectiveness", "successful", "identifying limitations", "used in practice/popular", "applies to real world", "practical", "useful", "building on past work"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "39b7007e6f3dd0744833f292f07ed77973503bfd", "title": "Data-Efficient Hierarchical Reinforcement Learning", "authors": ["Ofir Nachum", "Shixiang Gu", "Honglak Lee", "Sergey Levine"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "NeurIPS-2018-data-efficient-hierarchical-reinforcement-learning-Paper.pdf", "abstract": "Hierarchical reinforcement learning (HRL) is a promising approach to extend traditional reinforcement learning (RL) methods to solve more complex tasks. Yet, the majority of current HRL methods require careful task-speciﬁc design and on-policy training, making them difﬁcult to apply in real-world scenarios. In this paper, we study how we can develop HRL algorithms that are general, in that they do not make onerous additional assumptions beyond standard RL algorithms, and efﬁcient, in the sense that they can be used with modest numbers of interaction samples, making them suitable for real-world problems such as robotic control. For generality, we develop a scheme where lower-level controllers are supervised with goals that are learned and proposed automatically by the higher-level controllers. To address efﬁciency, we propose to use off-policy experience for both higher- and lower-level training. This poses a considerable challenge, since changes to the lower-level behaviors change the action space for the higher-level policy, and we introduce an off-policy correction to remedy this challenge. This allows us to take advantage of recent advances in off-policy model-free RL to learn both higher- and lower-level policies using substantially fewer environment interactions than on-policy algorithms. We term the resulting HRL agent HIRO and ﬁnd that it is generally applicable and highly sample-efﬁcient. Our experiments show that HIRO can be used to learn highly complex behaviors for simulated robots, such as pushing objects and utilizing them to reach target locations,1 learning from only a few million samples, equivalent to a few days of real-time interaction. In comparisons with a number of prior HRL methods, we ﬁnd that our approach substantially outperforms previous state-of-the-art techniques.2", "values": ["novelty", "simplicity", "robustness", "scientific methodology", "promising", "generality", "automatic", "performance", "efficiency", "unifying ideas or integrating components", "applies to real world", "building on past work"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library", "authors": ["Adam Paszke", "Sam Gross", "Francisco Massa", "Adam Lerer", "James Bradbury", "Gregory Chanan", "Trevor Killeen", "Zeming Lin", "Natalia Gimelshein", "Luca Antiga", "Alban Desmaison", "Andreas Köpf", "Edward Z. Yang", "Zachary Devito", "Martin Raison", "Alykhan Tejani", "Sasank Chilamkurthy", "Benoit Steiner", "Lu Fang", "Junjie Bai", "Soumith Chintala"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "NeurIPS-2019-pytorch-an-imperative-style-high-performance-deep-learning-library-Paper.pdf", "abstract": "Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientiﬁc computing libraries, while remaining efﬁcient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reﬂected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efﬁciency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks. 33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.", "values": ["generalization", "qualitative evidence (e.g. examples)", "controllability (of model owner)", "principled", "performance", "efficiency", "used in practice/popular", "easy to implement", "parallelizability / distributed", "facilitating use (e.g. sharing code)", "scales up", "applies to real world", "building on past work"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "413263b022ab9ba239a4dd7887d3eeaba0fca258", "title": "Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity", "authors": ["Byron M Yu", "John P Cunningham", "Gopal Santhanam", "Stephen I Ryu", "Krishna V Shenoy", "Maneesh Sahani"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "NIPS-2008-gaussian-process-factor-analysis-for-low-dimensional-single-trial-analysis-of-neural-population-activity-Paper.pdf", "abstract": "We consider the problem of extracting smooth, low-dimensional neural trajecto- ries that summarize the activity recorded simultaneously from tens to hundreds of neurons on individual experimental trials. Current methods for extracting neural trajectories involve a two-stage process: the data are ﬁrst “denoised” by smooth- ing over time, then a static dimensionality reduction technique is applied. We ﬁrst describe extensions of the two-stage methods that allow the degree of smoothing to be chosen in a principled way, and account for spiking variability that may vary both across neurons and across time. We then present a novel method for extract- ing neural trajectories, Gaussian-process factor analysis (GPFA), which uniﬁes the smoothing and dimensionality reduction operations in a common probabilis- tic framework. We applied these methods to the activity of 61 neurons recorded simultaneously in macaque premotor and motor cortices during reach planning and execution. By adopting a goodness-of-ﬁt metric that measures how well the activity of each neuron can be predicted by all other recorded neurons, we found that GPFA provided a better characterization of the population activity than the two-stage methods.", "values": ["novelty", "simplicity", "generalization", "robustness", "controllability (of model owner)", "principled", "performance", "unifying ideas or integrating components", "identifying limitations", "improvement", "easy to implement", "building on past work"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "42ec3db12a2e4628885451b13035c2e975220a25", "title": "A Convergence Theory for Deep Learning via Over-Parameterization", "authors": ["Zeyuan Allen-Zhu", "Yuanzhi Li", "Zhao Song"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "allen-zhu19a.pdf", "abstract": "Deep neural networks (DNNs) have demon- strated dominating performance in many ﬁelds; since AlexNet, networks used in practice are go- ing wider and deeper. On the theoretical side, a long line of works have been focusing on why we can train neural networks when there is only one hidden layer. The theory of multi-layer net- works remains unsettled. In this work, we prove simple algorithms such as stochastic gradient de- scent (SGD) can ﬁnd global minima on the train- ing objective of DNNs in polynomial time. We only make two assumptions: the inputs do not de- generate and the network is over-parameterized. The latter means the number of hidden neurons is sufﬁciently large: polynomial in L, the num- ber of DNN layers and in n, the number of train- ing samples. As concrete examples, starting from randomly initialized weights, we show that SGD attains 100% training accuracy in classiﬁcation tasks, or minimizes regression loss in linear con- vergence speed ε ∝e−Ω(T ), with running time polynomial in n and L. Our theory applies to the widely-used but non-smooth ReLU activation, and to any smooth and possibly non-convex loss functions. In terms of network architectures, our theory at least applies to fully-connected neural networks, convolutional neural networks (CNN), and residual neural networks (ResNet). *Equal contribution . Full version and future updates are avail- able at https://arxiv.org/abs/1811.03962. This paper is a follow up to the recurrent neural network (RNN) paper (Allen-Zhu et al., 2018b) by the same set of authors. Most of the techniques used in this paper were already discovered in the RNN paper, and this paper can be viewed as a simpliﬁcation (or to some extent a special case) of the RNN setting in order to reach out to a wider audience. We compare the difference and mention our additional contribution in Section 1.2. 1Microsoft Research AI 2Stanford University 3Princeton University 4UT-Austin 5University Wash- ington 6Harvard University. Correspondence to: Zeyuan Allen-Zhu <zeyuan@csail.mit.edu>, Yuanzhi Li <yuanzhil@stanford.edu>, Zhao Song <zhaos@utexas.edu>. Proceedings of the 36 th International Conference on Machine Learning, Long Beach, California, PMLR 97, 2019. Copyright 2019 by the author(s).", "values": ["simplicity", "generalization", "formal description/analysis", "theoretical guarantees", "quantitative evidence (e.g. experiments)", "concreteness", "performance", "efficiency", "successful", "unifying ideas or integrating components", "identifying limitations", "critique", "understanding (for researchers)", "used in practice/popular", "practical", "building on past work"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "449310e3538b08b43227d660227dfd2875c3c3c1", "title": "Neural Ordinary Differential Equations", "authors": ["Ricky T. Q. Chen", "Yulia Rubanova", "Jesse Bettencourt", "David Duvenaud"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "NeurIPS-2018-neural-ordinary-differential-equations-Paper.pdf", "abstract": "We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a black- box differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing ﬂows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.", "values": ["novelty", "simplicity", "generalization", "realistic output", "formal description/analysis", "theoretical guarantees", "approximation", "quantitative evidence (e.g. experiments)", "scientific methodology", "controllability (of model owner)", "large scale", "preciseness", "automatic", "performance", "efficiency", "requires few resources", "parallelizability / distributed", "scales up", "beneficence", "building on past work"], "labels_source": "Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "44b3b3bb40a9055eccdf86ea1702f6ae8b38934c", "title": "Decentralized Stochastic Optimization and Gossip Algorithms with Compressed Communication", "authors": ["Anastasiia Koloskova", "Sebastian Urban Stich", "Martin Jaggi"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "koloskova19a.pdf", "abstract": "We consider decentralized stochastic optimiza- tion with the objective function (e.g. data samples for machine learning tasks) being distributed over n machines that can only communicate to their neighbors on a ﬁxed communication graph. To address the communication bottleneck, the nodes compress (e.g. quantize or sparsify) their model updates. We cover both unbiased and biased com- pression operators with quality denoted by δ ≤1 (δ = 1 meaning no compression). We (i) propose a novel gossip-based stochastic gradient descent algorithm, CHOCO-SGD, that converges at rate O \u00001/(nT) + 1/(Tρ2δ)2\u0001 for strongly convex objectives, where T denotes the number of iterations and ρ the eigengap of the connectivity matrix. We (ii) present a novel gossip algorithm, CHOCO-GOSSIP, for the av- erage consensus problem that converges in time O(1/(ρ2δ) log(1/ε)) for accuracy ε > 0. This is (up to our knowledge) the ﬁrst gossip algorithm that supports arbitrary compressed messages for δ > 0 and still exhibits linear convergence. We (iii) show in experiments that both of our algo- rithms do outperform the respective state-of-the- art baselines and CHOCO-SGD can reduce com- munication by at least two orders of magnitudes.", "values": ["novelty", "generalization", "formal description/analysis", "theoretical guarantees", "quantitative evidence (e.g. experiments)", "scientific methodology", "large scale", "promising", "generality", "exactness", "preciseness", "performance", "efficiency", "improvement", "used in practice/popular", "parallelizability / distributed", "scales up", "applies to real world", "privacy", "building on past work"], "labels_source": "Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "4b5744dd44a0026c6f386d5cb21b795499d5efb7", "title": "Generalization Bounds of Stochastic Gradient Descent for Wide and Deep Neural Networks", "authors": ["Yuan Cao", "Quanquan Gu"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "NeurIPS-2019-generalization-bounds-of-stochastic-gradient-descent-for-wide-and-deep-neural-networks-Paper.pdf", "abstract": "We study the training and generalization of deep neural networks (DNNs) in the over-parameterized regime, where the network width (i.e., number of hidden nodes per layer) is much larger than the number of training data points. We show that, the expected 0-1 loss of a wide enough ReLU network trained with stochastic gradient descent (SGD) and random initialization can be bounded by the training loss of a random feature model induced by the network gradient at initialization, which we call a neural tangent random feature (NTRF) model. For data distributions that can be classiﬁed by NTRF model with sufﬁciently small error, our result yields a generalization error bound in the order of r Opn´1{2q that is independent of the network width. Our result is more general and sharper than many existing generalization error bounds for over-parameterized neural networks. In addition, we establish a strong connection between our generalization error bound and the neural tangent kernel (NTK) proposed in recent work.", "values": ["generalization", "formal description/analysis", "scientific methodology", "generality", "performance", "successful", "understanding (for researchers)", "building on past work"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "4c746fb88a4b141ec3bca06504e800bf392ce170", "title": "Listwise approach to learning to rank: theory and algorithm", "authors": ["Fen Xia", "Tie-Yan Liu", "Jue Wang", "Wensheng Zhang", "Hang Li"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "1390156.1390306.pdf", "abstract": "This paper aims to conduct a study on the listwise approach to learning to rank. The listwise approach learns a ranking function by taking individual lists as instances and min- imizing a loss function deﬁned on the pre- dicted list and the ground-truth list. Exist- ing work on the approach mainly focused on the development of new algorithms; methods such as RankCosine and ListNet have been proposed and good performances by them have been observed. Unfortunately, the un- derlying theory was not suﬃciently studied so far. To amend the problem, this paper proposes conducting theoretical analysis of learning to rank algorithms through inves- tigations on the properties of the loss func- tions, including consistency, soundness, con- tinuity, diﬀerentiability, convexity, and eﬃ- ciency. A suﬃcient condition on consistency for ranking is given, which seems to be the ﬁrst such result obtained in related research. The paper then conducts analysis on three loss functions: likelihood loss, cosine loss, and cross entropy loss. The latter two were used in RankCosine and ListNet. The use of the likelihood loss leads to the development of Appearing in Proceedings of the 25 th International Confer- ence on Machine Learning, Helsinki, Finland, 2008. Copy- right 2008 by the author(s)/owner(s). *The work was performed when the ﬁrst author was an intern at Microsoft Research Asia. a new listwise method called ListMLE, whose loss function oﬀers better properties, and also leads to better experimental results.", "values": ["novelty", "generalization", "formal description/analysis", "theoretical guarantees", "quantitative evidence (e.g. experiments)", "performance", "efficiency", "effectiveness", "successful", "identifying limitations", "understanding (for researchers)", "used in practice/popular", "applies to real world", "building on past work"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "4debb99c0c63bfaa97dd433bc2828e4dac81c48b", "title": "Addressing Function Approximation Error in Actor-Critic Methods", "authors": ["Scott Fujimoto", "Herke Van Hoof", "David Meger"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "fujimoto18a.pdf", "abstract": "In value-based reinforcement learning methods such as deep Q-learning, function approximation errors are known to lead to overestimated value estimates and suboptimal policies. We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and the critic. Our algorithm builds on Double Q-learning, by taking the mini- mum value between a pair of critics to limit over- estimation. We draw the connection between tar- get networks and overestimation bias, and suggest delaying policy updates to reduce per-update error and further improve performance. We evaluate our method on the suite of OpenAI gym tasks, outperforming the state of the art in every envi- ronment tested.", "values": ["novelty", "generalization", "approximation", "quantitative evidence (e.g. experiments)", "scientific methodology", "preciseness", "performance", "efficiency", "unifying ideas or integrating components", "identifying limitations", "understanding (for researchers)", "easy to implement", "building on past work"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "4e07791ee0872401215f12aefde342bd843240cc", "title": "Nonparametric Latent Feature Models for Link Prediction", "authors": ["Kurt Miller", "Michael I. Jordan", "Thomas L. Griffiths"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "NIPS-2009-nonparametric-latent-feature-models-for-link-prediction-Paper.pdf", "abstract": "As the availability and importance of relational data—such as the friendships sum- marized on a social networking website—increases, it becomes increasingly im- portant to have good models for such data. The kinds of latent structure that have been considered for use in predicting links in such networks have been relatively limited. In particular, the machine learning community has focused on latent class models, adapting Bayesian nonparametric methods to jointly infer how many la- tent classes there are while learning which entities belong to each class. We pursue a similar approach with a richer kind of latent variable—latent features—using a Bayesian nonparametric approach to simultaneously infer the number of features at the same time we learn which entities have each feature. Our model combines these inferred features with known covariates in order to perform link prediction. We demonstrate that the greater expressiveness of this approach allows us to im- prove performance on three datasets.", "values": ["novelty", "generalization", "quantitative evidence (e.g. experiments)", "performance", "efficiency", "successful", "scales up", "applies to real world", "beneficence", "building on past work"], "labels_source": "Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "4e0bb8c1c683b43357c5d5216f6b74ff2cb32434", "title": "Do ImageNet Classifiers Generalize to ImageNet?", "authors": ["Benjamin Recht", "Rebecca Roelofs", "Ludwig Schmidt", "Vaishaal Shankar"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "recht19a.pdf", "abstract": "We build new test sets for the CIFAR-10 and Ima- geNet datasets. Both benchmarks have been the focus of intense research for almost a decade, rais- ing the danger of overﬁtting to excessively re-used test sets. By closely following the original dataset creation processes, we test to what extent current classiﬁcation models generalize to new data. We evaluate a broad range of models and ﬁnd accu- racy drops of 3% – 15% on CIFAR-10 and 11% – 14% on ImageNet. However, accuracy gains on the original test sets translate to larger gains on the new test sets. Our results suggest that the accuracy drops are not caused by adaptivity, but by the models’ inability to generalize to slightly “harder” images than those found in the original test sets.", "values": ["novelty", "generalization", "robustness", "approximation", "quantitative evidence (e.g. experiments)", "qualitative evidence (e.g. examples)", "human-like mechanism", "promising", "exactness", "performance", "efficiency", "effectiveness", "identifying limitations", "critique", "understanding (for researchers)", "improvement", "progress", "reproducibility", "applies to real world", "learning from humans", "deferral to humans", "building on past work"], "labels_source": "Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "4eeac07324e5b15bca90295c66b21c05726ea2bb", "title": "Learning Non-Linear Combinations of Kernels", "authors": ["Corinna Cortes", "Mehryar Mohri", "Mehryar Mohri", "Afshin Rostamizadeh", "Afshin Rostamizadeh"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "NIPS-2009-learning-non-linear-combinations-of-kernels-Paper.pdf", "abstract": "This paper studies the general problem of learning kernels based on a polynomial combination of base kernels. We analyze this problem in the case of regression and the kernel ridge regression algorithm. We examine the corresponding learning kernel optimization problem, show how that minimax problem can be reduced to a simpler minimization problem, and prove that the global solution of this problem always lies on the boundary. We give a projection-based gradient descent algo- rithm for solving the optimization problem, shown empirically to converge in few iterations. Finally, we report the results of extensive experiments with this algo- rithm using several publicly available datasets demonstrating the effectiveness of our technique.", "values": ["simplicity", "generalization", "quantitative evidence (e.g. experiments)", "scientific methodology", "generality", "performance", "efficiency", "effectiveness", "successful", "identifying limitations", "understanding (for researchers)", "improvement", "progress", "parallelizability / distributed", "facilitating use (e.g. sharing code)", "beneficence", "building on past work"], "labels_source": "Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9", "title": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks", "authors": ["Mingxing Tan", "Quoc V. Le"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "tan19a.pdf", "abstract": "Convolutional Neural Networks (ConvNets) are commonly developed at a ﬁxed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we sys- tematically study model scaling and identify that carefully balancing network depth, width, and res- olution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefﬁcient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called Efﬁcient- Nets, which achieve much better accuracy and efﬁ- ciency than previous ConvNets. In particular, our EfﬁcientNet-B7 achieves state-of-the-art 84.4% top-1 / 97.1% top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfﬁcientNets also transfer well and achieve state-of-the-art ac- curacy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters.", "values": ["simplicity", "generalization", "formal description/analysis", "quantitative evidence (e.g. experiments)", "principled", "performance", "efficiency", "effectiveness", "understanding (for researchers)", "easy to implement", "requires few resources", "facilitating use (e.g. sharing code)", "scales up"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "5262fe8369992259be27165ccd09d1d31c7a4def", "title": "Bayesian probabilistic matrix factorization using Markov chain Monte Carlo", "authors": ["Ruslan Salakhutdinov", "Andriy Mnih"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "1390156.1390267.pdf", "abstract": "Low-rank matrix approximation methods provide one of the simplest and most eﬀective approaches to collaborative ﬁltering. Such models are usually ﬁtted to data by ﬁnding a MAP estimate of the model parameters, a procedure that can be performed eﬃciently even on very large datasets. However, un- less the regularization parameters are tuned carefully, this approach is prone to overﬁt- ting because it ﬁnds a single point estimate of the parameters. In this paper we present a fully Bayesian treatment of the Probabilistic Matrix Factorization (PMF) model in which model capacity is controlled automatically by integrating over all model parameters and hyperparameters. We show that Bayesian PMF models can be eﬃciently trained us- ing Markov chain Monte Carlo methods by applying them to the Netﬂix dataset, which consists of over 100 million movie ratings. The resulting models achieve signiﬁcantly higher prediction accuracy than PMF models trained using MAP estimation.", "values": ["novelty", "simplicity", "large scale", "automatic", "performance", "efficiency", "effectiveness", "successful", "identifying limitations", "understanding (for researchers)", "used in practice/popular", "parallelizability / distributed", "scales up", "applies to real world", "building on past work"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "554fabcedec7ee5361661614c6b45dc5661a5f79", "title": "Efficient Large-Scale Distributed Training of Conditional Maximum Entropy Models", "authors": ["Ryan Mcdonald", "Mehryar Mohri", "Nathan Silberman", "Dan Walker", "Gideon S. Mann"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "NIPS-2009-efficient-large-scale-distributed-training-of-conditional-maximum-entropy-models-Paper.pdf", "abstract": "Training conditional maximum entropy models on massive data sets requires sig- niﬁcant computational resources. We examine three common distributed training methods for conditional maxent: a distributed gradient computation method, a majority vote method, and a mixture weight method. We analyze and compare the CPU and network time complexity of each of these methods and present a theoret- ical analysis of conditional maxent models, including a study of the convergence of the mixture weight method, the most resource-efﬁcient technique. We also re- port the results of large-scale experiments comparing these three methods which demonstrate the beneﬁts of the mixture weight method: this method consumes less resources, while achieving a performance comparable to that of standard ap- proaches.", "values": [], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "57458bc1cffe5caa45a885af986d70f723f406b4", "title": "A unified architecture for natural language processing:deep neural networks with multitask learning", "authors": ["Ronan Collobert", "Jason Weston"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "1390156.1390177.pdf", "abstract": "We describe a single convolutional neural net- work architecture that, given a sentence, out- puts a host of language processing predic- tions: part-of-speech tags, chunks, named en- tity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense (grammatically and semanti- cally) using a language model. The entire network is trained jointly on all these tasks using weight-sharing, an instance of multitask learning. All the tasks use labeled data ex- cept the language model which is learnt from unlabeled text and represents a novel form of semi-supervised learning for the shared tasks. We show how both multitask learning and semi-supervised learning improve the general- ization of the shared tasks, resulting in state- of-the-art performance.", "values": ["novelty", "generalization", "quantitative evidence (e.g. experiments)", "large scale", "generality", "performance", "efficiency", "unifying ideas or integrating components", "understanding (for researchers)", "used in practice/popular", "applies to real world", "interpretable (to users)", "building on past work"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "5a2668bf420d8509a4dfa28e1cdcdac14c649975", "title": "3D Object Recognition with Deep Belief Nets", "authors": ["Vinod Nair", "Geoffrey E. Hinton"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "NIPS-2009-3d-object-recognition-with-deep-belief-nets-Paper.pdf", "abstract": "We introduce a new type of top-level model for Deep Belief Nets and evalu- ate it on a 3D object recognition task. The top-level model is a third-order Boltzmann machine, trained using a hybrid algorithm that combines both generative and discriminative gradients. Performance is evaluated on the NORB database (normalized-uniform version), which contains stereo-pair images of objects under diﬀerent lighting conditions and viewpoints. Our model achieves 6.5% error on the test set, which is close to the best pub- lished result for NORB (5.9%) using a convolutional neural net that has built-in knowledge of translation invariance. It substantially outperforms shallow models such as SVMs (11.6%). DBNs are especially suited for semi-supervised learning, and to demonstrate this we consider a modiﬁed version of the NORB recognition task in which additional unlabeled images are created by applying small translations to the images in the database. With the extra unlabeled data (and the same amount of labeled data as before), our model achieves 5.2% error.", "values": ["novelty", "generalization", "quantitative evidence (e.g. experiments)", "performance", "efficiency", "unifying ideas or integrating components", "applies to real world", "useful", "building on past work"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "5ae20e0bdfddc1888148e0fcde88d937e96318d2", "title": "Learning structural SVMs with latent variables", "authors": ["Chun-Nam John Yu", "Thorsten Joachims"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "1553374.1553523.pdf", "abstract": "We present a large-margin formulation and algorithm for structured output prediction that allows the use of latent variables. Our proposal covers a large range of applica- tion problems, with an optimization problem that can be solved eﬃciently using Concave- Convex Programming. The generality and performance of the approach is demonstrated through three applications including motif- ﬁnding, noun-phrase coreference resolution, and optimizing precision at k in information retrieval.", "values": ["generalization", "formal description/analysis", "qualitative evidence (e.g. examples)", "generality", "performance", "efficiency", "useful", "building on past work"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "611fe6e34df07ea1b2104899e49642b4531b53e9", "title": "Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers", "authors": ["Zeyuan Allen-Zhu", "Yuanzhi Li", "Yingyu Liang"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "NeurIPS-2019-learning-and-generalization-in-overparameterized-neural-networks-going-beyond-two-layers-Paper.pdf", "abstract": "The fundamental learning theory behind neural networks remains largely open. What classes of functions can neural networks actually learn? Why doesn’t the trained network overﬁt when it is overparameterized? In this work, we prove that overparameterized neural networks can learn some notable concept classes, including two and three-layer networks with fewer pa- rameters and smooth activations. Moreover, the learning can be simply done by SGD (stochastic gradient descent) or its variants in polynomial time using poly- nomially many samples. The sample complexity can also be almost independent of the number of parameters in the network. On the technique side, our analysis goes beyond the so-called NTK (neural tan- gent kernel) linearization of neural networks in prior works. We establish a new notion of quadratic approximation of the neural network, and connect it to the SGD theory of escaping saddle points.", "values": ["novelty", "simplicity", "generalization", "formal description/analysis", "theoretical guarantees", "approximation", "quantitative evidence (e.g. experiments)", "scientific methodology", "performance", "efficiency", "successful", "unifying ideas or integrating components", "understanding (for researchers)", "used in practice/popular", "applies to real world", "useful", "building on past work"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "6400c36efdb8a66b401b6aef26c057227266fddd", "title": "PointCNN: Convolution On X-Transformed Points", "authors": ["Yangyan Li", "Rui Bu", "Mingchao Sun", "Wei Wu", "Xinhan Di", "Baoquan Chen"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "NeurIPS-2018-pointcnn-convolution-on-x-transformed-points-Paper.pdf", "abstract": "We present a simple and general framework for feature learning from point clouds. The key to the success of CNNs is the convolution operator that is capable of leveraging spatially-local correlation in data represented densely in grids (e.g. im- ages). However, point clouds are irregular and unordered, thus directly convolving kernels against features associated with the points will result in desertion of shape information and variance to point ordering. To address these problems, we propose to learn an X-transformation from the input points to simultaneously promote two causes: the ﬁrst is the weighting of the input features associated with the points, and the second is the permutation of the points into a latent and potentially canonical order. Element-wise product and sum operations of the typical convolution operator are subsequently applied on the X-transformed features. The proposed method is a generalization of typical CNNs to feature learning from point clouds, thus we call it PointCNN. Experiments show that PointCNN achieves on par or better performance than state-of-the-art methods on multiple challenging benchmark datasets and tasks.", "values": ["simplicity", "scientific methodology", "generality", "performance", "effectiveness", "understanding (for researchers)", "facilitating use (e.g. sharing code)", "applies to real world", "building on past work"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "64441c8396211b5e799b9ad5138dade15ff5cd0a", "title": "Grassmann discriminant analysis: a unifying view on subspace-based learning", "authors": ["Jihun Hamm", "Daniel D. Lee"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "1390156.1390204.pdf", "abstract": "In this paper we propose a discriminant learning framework for problems in which data consist of linear subspaces instead of vectors. By treating subspaces as basic el- ements, we can make learning algorithms adapt naturally to the problems with lin- ear invariant structures. We propose a uni- fying view on the subspace-based learning method by formulating the problems on the Grassmann manifold, which is the set of ﬁxed-dimensional linear subspaces of a Eu- clidean space. Previous methods on the prob- lem typically adopt an inconsistent strategy: feature extraction is performed in the Eu- clidean space while non-Euclidean distances are used. In our approach, we treat each sub- space as a point in the Grassmann space, and perform feature extraction and classiﬁcation in the same space. We show feasibility of the approach by using the Grassmann kernel functions such as the Projection kernel and the Binet-Cauchy kernel. Experiments with real image databases show that the proposed method performs well compared with state- of-the-art algorithms.", "values": ["novelty", "simplicity", "generalization", "robustness", "formal description/analysis", "theoretical guarantees", "approximation", "quantitative evidence (e.g. experiments)", "performance", "efficiency", "unifying ideas or integrating components", "understanding (for researchers)", "applies to real world", "building on past work"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "651adaa058f821a890f2c5d1053d69eb481a8352", "title": "Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples", "authors": ["Anish Athalye", "Nicholas Carlini", "David Wagner"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "athalye18a.pdf", "abstract": "We identify obfuscated gradients, a kind of gradi- ent masking, as a phenomenon that leads to a false sense of security in defenses against adversarial examples. While defenses that cause obfuscated gradients appear to defeat iterative optimization- based attacks, we ﬁnd defenses relying on this effect can be circumvented. We describe charac- teristic behaviors of defenses exhibiting the effect, and for each of the three types of obfuscated gra- dients we discover, we develop attack techniques to overcome it. In a case study, examining non- certiﬁed white-box-secure defenses at ICLR 2018, we ﬁnd obfuscated gradients are a common occur- rence, with 7 of 9 defenses relying on obfuscated gradients. Our new attacks successfully circum- vent 6 completely, and 1 partially, in the original threat model each paper considers.", "values": ["novelty", "robustness", "formal description/analysis", "approximation", "qualitative evidence (e.g. examples)", "scientific methodology", "generality", "performance", "successful", "identifying limitations", "understanding (for researchers)", "progress", "used in practice/popular", "facilitating use (e.g. sharing code)", "applies to real world", "security", "building on past work"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "65a9c7b0800c86a196bc14e7621ff895cc6ab287", "title": "ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks", "authors": ["Jiasen Lu", "Dhruv Batra", "Dhruv Batra", "Devi Parikh", "Devi Parikh", "Stefan Lee", "Stefan Lee"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "NeurIPS-2019-vilbert-pretraining-task-agnostic-visiolinguistic-representations-for-vision-and-language-tasks-Paper.pdf", "abstract": "We present ViLBERT (short for Vision-and-Language BERT), a model for learning task-agnostic joint representations of image content and natural language. We extend the popular BERT architecture to a multi-modal two-stream model, pro- cessing both visual and textual inputs in separate streams that interact through co-attentional transformer layers. We pretrain our model through two proxy tasks on the large, automatically collected Conceptual Captions dataset and then transfer it to multiple established vision-and-language tasks – visual question answering, visual commonsense reasoning, referring expressions, and caption-based image retrieval – by making only minor additions to the base architecture. We observe signiﬁcant improvements across tasks compared to existing task-speciﬁc models – achieving state-of-the-art on all four tasks. Our work represents a shift away from learning groundings between vision and language only as part of task training and towards treating visual grounding as a pretrainable and transferable capability.", "values": ["novelty", "generalization", "quantitative evidence (e.g. experiments)", "large scale", "promising", "automatic", "performance", "efficiency", "unifying ideas or integrating components", "identifying limitations", "critique", "understanding (for researchers)", "used in practice/popular", "easy to implement", "facilitating use (e.g. sharing code)", "useful", "security", "easy to work with", "building on past work"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "6b4ca249b3b28d3fee65f69714440c08d42cee64", "title": "Which Training Methods for GANs do actually Converge?", "authors": ["Lars Mescheder", "Andreas Geiger", "Andreas Geiger", "Sebastian Nowozin"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "mescheder18a.pdf", "abstract": "Recent work has shown local convergence of GAN training for absolutely continuous data and generator distributions. In this paper, we show that the requirement of absolute continuity is nec- essary: we describe a simple yet prototypical counterexample showing that in the more real- istic case of distributions that are not absolutely continuous, unregularized GAN training is not always convergent. Furthermore, we discuss reg- ularization strategies that were recently proposed to stabilize GAN training. Our analysis shows that GAN training with instance noise or zero- centered gradient penalties converges. On the other hand, we show that Wasserstein-GANs and WGAN-GP with a ﬁnite number of discriminator updates per generator update do not always con- verge to the equilibrium point. We discuss these results, leading us to a new explanation for the stability problems of GAN training. Based on our analysis, we extend our convergence results to more general GANs and prove local conver- gence for simpliﬁed gradient penalties even if the generator and data distributions lie on lower di- mensional manifolds. We ﬁnd these penalties to work well in practice and use them to learn high- resolution generative image models for a variety of datasets with little hyperparameter tuning.", "values": ["novelty", "simplicity", "generalization", "robustness", "realistic output", "formal description/analysis", "theoretical guarantees", "quantitative evidence (e.g. experiments)", "qualitative evidence (e.g. examples)", "scientific methodology", "generality", "identifying limitations", "understanding (for researchers)", "used in practice/popular", "facilitating use (e.g. sharing code)", "applies to real world", "practical", "useful", "realistic world model", "building on past work"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "6c405d4b5dc41a86be05acd59c06ed19daf01d14", "title": "Theoretically Principled Trade-off between Robustness and Accuracy", "authors": ["Hongyang Zhang", "Yaodong Yu", "Jiantao Jiao", "Eric P. Xing", "Laurent El Ghaoui", "Michael I. Jordan"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "zhang19p.pdf", "abstract": "We identify a trade-off between robustness and accuracy that serves as a guiding principle in the design of defenses against adversarial examples. Although this problem has been widely studied empirically, much remains unknown concerning the theory underlying this trade-off. In this work, we decompose the prediction error for adversarial examples (robust error) as the sum of the natural (classiﬁcation) error and boundary error, and pro- vide a differentiable upper bound using the theory of classiﬁcation-calibrated loss, which is shown to be the tightest possible upper bound uniform over all probability distributions and measurable pre- dictors. Inspired by our theoretical analysis, we also design a new defense method, TRADES, to trade adversarial robustness off against accuracy. Our proposed algorithm performs well experimen- tally in real-world datasets. The methodology is the foundation of our entry to the NeurIPS 2018 Adversarial Vision Challenge in which we won the 1st place out of ~2,000 submissions, surpass- ing the runner-up approach by 11.41% in terms of mean `2 perturbation distance.", "values": ["novelty", "simplicity", "generalization", "robustness", "formal description/analysis", "theoretical guarantees", "approximation", "quantitative evidence (e.g. experiments)", "qualitative evidence (e.g. examples)", "scientific methodology", "large scale", "generality", "performance", "effectiveness", "identifying limitations", "understanding (for researchers)", "improvement", "scales up", "applies to real world", "beneficence", "security", "building on past work"], "labels_source": "Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "6e4fd9b4b2b673c981cda528d8039a221ad35225", "title": "NAS-Bench-101: Towards Reproducible Neural Architecture Search", "authors": ["Chris Ying", "Aaron Klein", "Eric Christiansen", "Esteban Real", "Kevin Murphy", "Frank Hutter"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "ying19a.pdf", "abstract": "Recent advances in neural architecture search (NAS) demand tremendous computational re- sources, which makes it difﬁcult to reproduce experiments and imposes a barrier-to-entry to re- searchers without access to large-scale computa- tion. We aim to ameliorate these problems by in- troducing NAS-Bench-101, the ﬁrst public archi- tecture dataset for NAS research. To build NAS- Bench-101, we carefully constructed a compact, yet expressive, search space, exploiting graph iso- morphisms to identify 423k unique convolutional architectures. We trained and evaluated all of these architectures multiple times on CIFAR-10 and compiled the results into a large dataset of over 5 million trained models. This allows re- searchers to evaluate the quality of a diverse range of models in milliseconds by querying the pre- computed dataset. We demonstrate its utility by analyzing the dataset as a whole and by bench- marking a range of architecture optimization al- gorithms.", "values": ["novelty", "generalization", "quantitative evidence (e.g. experiments)", "large scale", "promising", "exactness", "performance", "efficiency", "understanding (for researchers)", "reproducibility", "requires few resources", "facilitating use (e.g. sharing code)", "scales up", "building on past work"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "73d6a26f407db77506959fdf3f7b853e44f3844a", "title": "Training restricted Boltzmann machines using approximations to the likelihood gradient", "authors": ["Tijmen Tieleman"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "1390156.1390290.pdf", "abstract": "A new algorithm for training Restricted Boltzmann Machines is introduced. The al- gorithm, named Persistent Contrastive Di- vergence, is diﬀerent from the standard Con- trastive Divergence algorithms in that it aims to draw samples from almost exactly the model distribution. It is compared to some standard Contrastive Divergence and Pseudo-Likelihood algorithms on the tasks of modeling and classifying various types of data. The Persistent Contrastive Divergence algorithm outperforms the other algorithms, and is equally fast and simple.", "values": ["novelty", "simplicity", "generalization", "formal description/analysis", "theoretical guarantees", "approximation", "quantitative evidence (e.g. experiments)", "performance", "efficiency", "understanding (for researchers)", "used in practice/popular", "applies to real world", "building on past work"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "77e379fd57ea44638fc628623e383eccada82689", "title": "Kernel Methods for Deep Learning", "authors": ["Youngmin Cho", "Lawrence K. Saul"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "NIPS-2009-kernel-methods-for-deep-learning-Paper.pdf", "abstract": "We introduce a new family of positive-deﬁnite kernel functions that mimic the computation in large, multilayer neural nets. These kernel functions can be used in shallow architectures, such as support vector machines (SVMs), or in deep kernel-based architectures that we call multilayer kernel machines (MKMs). We evaluate SVMs and MKMs with these kernel functions on problems designed to illustrate the advantages of deep architectures. On several problems, we obtain better results than previous, leading benchmarks from both SVMs with Gaussian kernels as well as deep belief nets.", "values": ["novelty", "simplicity", "generalization", "quantitative evidence (e.g. experiments)", "scientific methodology", "large scale", "performance", "efficiency", "successful", "unifying ideas or integrating components", "identifying limitations", "understanding (for researchers)", "used in practice/popular", "easy to implement", "easy to work with", "building on past work"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "7a84a692327534fd227fa1e07fcb3816b633c591", "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks", "authors": ["Arthur Jacot", "Franck Gabriel", "Franck Gabriel", "Clément Hongler"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "NeurIPS-2018-neural-tangent-kernel-convergence-and-generalization-in-neural-networks-Paper.pdf", "abstract": "At initialization, artiﬁcial neural networks (ANNs) are equivalent to Gaussian processes in the inﬁnite-width limit (12; 9), thus connecting them to kernel methods. We prove that the evolution of an ANN during training can also be described by a kernel: during gradient descent on the parameters of an ANN, the network function fθ (which maps input vectors to output vectors) follows the kernel gradient of the functional cost (which is convex, in contrast to the parameter cost) w.r.t. a new kernel: the Neural Tangent Kernel (NTK). This kernel is central to describe the generalization features of ANNs. While the NTK is random at initialization and varies during training, in the inﬁnite-width limit it converges to an explicit limiting kernel and it stays constant during training. This makes it possible to study the training of ANNs in function space instead of parameter space. Convergence of the training can then be related to the positive-deﬁniteness of the limiting NTK. We then focus on the setting of least-squares regression and show that in the inﬁnite- width limit, the network function fθ follows a linear differential equation during training. The convergence is fastest along the largest kernel principal components of the input data with respect to the NTK, hence suggesting a theoretical motivation for early stopping. Finally we study the NTK numerically, observe its behavior for wide networks, and compare it to the inﬁnite-width limit.", "values": ["novelty", "simplicity", "generalization", "formal description/analysis", "theoretical guarantees", "quantitative evidence (e.g. experiments)", "scientific methodology", "preciseness", "performance", "efficiency", "unifying ideas or integrating components", "understanding (for researchers)", "progress"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "7af09246bae1d2d9abada79f441ba25858c69ef9", "title": "Confidence-weighted linear classification", "authors": ["Mark Dredze", "Koby Crammer", "Fernando Pereira"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "1390156.1390190.pdf", "abstract": "We introduce conﬁdence-weighted linear clas- siﬁers, which add parameter conﬁdence infor- mation to linear classiﬁers. Online learners in this setting update both classiﬁer param- eters and the estimate of their conﬁdence. The particular online algorithms we study here maintain a Gaussian distribution over parameter vectors and update the mean and covariance of the distribution with each in- stance. Empirical evaluation on a range of NLP tasks show that our algorithm improves over other state of the art online and batch methods, learns faster in the online setting, and lends itself to better classiﬁer combina- tion after parallel training.", "values": ["novelty", "simplicity", "formal description/analysis", "approximation", "quantitative evidence (e.g. experiments)", "large scale", "exactness", "performance", "efficiency", "improvement", "used in practice/popular", "parallelizability / distributed", "practical", "beneficence"], "labels_source": "Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "7c22a6a07e89461178b794681c675b209332ee15", "title": "Error Feedback Fixes SignSGD and other Gradient Compression Schemes", "authors": ["Sai Praneeth Reddy Karimireddy", "Quentin Rebjock", "Sebastian Urban Stich", "Martin Jaggi"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "karimireddy19a.pdf", "abstract": "Sign-based algorithms (e.g. SIGNSGD) have been proposed as a biased gradient compression tech- nique to alleviate the communication bottleneck in training large neural networks across multi- ple workers. We show simple convex counter- examples where signSGD does not converge to the optimum. Further, even when it does converge, signSGD may generalize poorly when compared with SGD. These issues arise because of the bi- ased nature of the sign compression operator. We then show that using error-feedback, i.e. in- corporating the error made by the compression operator into the next step, overcomes these is- sues. We prove that our algorithm (EF-SGD) with arbitrary compression operator achieves the same rate of convergence as SGD without any additional assumptions. Thus EF-SGDachieves gradient compression for free. Our experiments thoroughly substantiate the theory.", "values": ["simplicity", "generalization", "formal description/analysis", "theoretical guarantees", "approximation", "quantitative evidence (e.g. experiments)", "qualitative evidence (e.g. examples)", "large scale", "performance", "efficiency", "identifying limitations", "understanding (for researchers)", "progress", "used in practice/popular", "parallelizability / distributed", "building on past work"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "7e71eedb078181873a56f2adcfef9dddaeb95602", "title": "Simplifying Graph Convolutional Networks", "authors": ["Felix Wu", "Tianyi Zhang", "Amauri Holanda De Souza", "Amauri Holanda De Souza", "Christopher Fifty", "Tao Yu", "Kilian Q. Weinberger"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "wu19e.pdf", "abstract": "Graph Convolutional Networks (GCNs) and their variants have experienced signiﬁcant attention and have become the de facto methods for learning graph representations. GCNs derive inspiration primarily from recent deep learning approaches, and as a result, may inherit unnecessary complex- ity and redundant computation. In this paper, we reduce this excess complexity through suc- cessively removing nonlinearities and collapsing weight matrices between consecutive layers. We theoretically analyze the resulting linear model and show that it corresponds to a ﬁxed low-pass ﬁlter followed by a linear classiﬁer. Notably, our experimental evaluation demonstrates that these simpliﬁcations do not negatively impact accuracy in many downstream applications. Moreover, the resulting model scales to larger datasets, is natu- rally interpretable, and yields up to two orders of magnitude speedup over FastGCN.", "values": ["simplicity", "generalization", "theoretical guarantees", "quantitative evidence (e.g. experiments)", "scientific methodology", "performance", "efficiency", "effectiveness", "understanding (for researchers)", "improvement", "facilitating use (e.g. sharing code)", "scales up", "applies to real world", "interpretable (to users)", "beneficence", "building on past work"], "labels_source": "Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "800683edc4b24f61c985c025bab02e34ebff293b", "title": "Nonrigid Structure from Motion in Trajectory Space", "authors": ["Ijaz Akhter", "Yaser Sheikh", "Sohaib Khan", "Takeo Kanade"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "NIPS-2008-nonrigid-structure-from-motion-in-trajectory-space-Paper.pdf", "abstract": "Existing approaches to nonrigid structure from motion assume that the instanta- neous 3D shape of a deforming object is a linear combination of basis shapes, which have to be estimated anew for each video sequence. In contrast, we pro- pose that the evolving 3D structure be described by a linear combination of basis trajectories. The principal advantage of this approach is that we do not need to estimate any basis vectors during computation. We show that generic bases over trajectories, such as the Discrete Cosine Transform (DCT) basis, can be used to compactly describe most real motions. This results in a signiﬁcant reduction in unknowns, and corresponding stability in estimation. We report empirical per- formance, quantitatively using motion capture data, and qualitatively on several video sequences exhibiting nonrigid motions including piece-wise rigid motion, partially nonrigid motion (such as a facial expression), and highly nonrigid motion (such as a person dancing).", "values": ["novelty", "generalization", "formal description/analysis", "approximation", "quantitative evidence (e.g. experiments)", "qualitative evidence (e.g. examples)", "performance", "efficiency", "unifying ideas or integrating components", "identifying limitations", "applies to real world", "realistic world model", "building on past work"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "80196cdfcd0c6ce2953bf65a7f019971e2026386", "title": "IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures", "authors": ["Lasse Espeholt", "Hubert Soyer", "Remi Munos", "Karen Simonyan", "Volodymir Mnih", "Tom Ward", "Yotam Doron", "Vlad Firoiu", "Tim Harley", "Iain Dunning", "Shane Legg", "Koray Kavukcuoglu"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "espeholt18a.pdf", "abstract": "In this work we aim to solve a large collection of tasks using a single reinforcement learning agent with a single set of parameters. A key challenge is to handle the increased amount of data and ex- tended training time. We have developed a new distributed agent IMPALA (Importance Weighted Actor-Learner Architecture) that not only uses resources more efﬁciently in single-machine train- ing but also scales to thousands of machines with- out sacriﬁcing data efﬁciency or resource utilisa- tion. We achieve stable learning at high through- put by combining decoupled acting and learning with a novel off-policy correction method called V-trace. We demonstrate the effectiveness of IM- PALA for multi-task reinforcement learning on DMLab-30 (a set of 30 tasks from the DeepMind Lab environment (Beattie et al., 2016)) and Atari- 57 (all available Atari games in Arcade Learning Environment (Bellemare et al., 2013a)). Our re- sults show that IMPALA is able to achieve better performance than previous agents with less data, and crucially exhibits positive transfer between tasks as a result of its multi-task approach.", "values": ["novelty", "simplicity", "generalization", "robustness", "quantitative evidence (e.g. experiments)", "large scale", "performance", "efficiency", "effectiveness", "unifying ideas or integrating components", "progress", "used in practice/popular", "parallelizability / distributed", "facilitating use (e.g. sharing code)", "scales up", "practical"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "804fb9542f4f56e264dd2df57c255a9a2011c00f", "title": "Adversarially Robust Generalization Requires More Data", "authors": ["Ludwig Schmidt", "Shibani Santurkar", "Dimitris Tsipras", "Kunal Talwar", "Aleksander Madry"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "NeurIPS-2018-adversarially-robust-generalization-requires-more-data-Paper.pdf", "abstract": "Machine learning models are often susceptible to adversarial perturbations of their inputs. Even small perturbations can cause state-of-the-art classiﬁers with high “standard” accuracy to produce an incorrect prediction with high conﬁdence. To better understand this phenomenon, we study adversarially robust learning from the viewpoint of generalization. We show that already in a simple natural data model, the sample complexity of robust learning can be signiﬁcantly larger than that of “standard” learning. This gap is information theoretic and holds irrespective of the training algorithm or the model family. We complement our theoretical results with experiments on popular image classiﬁcation datasets and show that a similar gap exists here as well. We postulate that the difﬁculty of training robust classiﬁers stems, at least partially, from this inherently larger sample complexity.", "values": ["simplicity", "generalization", "robustness", "formal description/analysis", "theoretical guarantees", "quantitative evidence (e.g. experiments)", "qualitative evidence (e.g. examples)", "scientific methodology", "exactness", "concreteness", "performance", "efficiency", "successful", "identifying limitations", "understanding (for researchers)", "progress", "used in practice/popular", "scales up", "applies to real world", "security", "easy to work with", "building on past work"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "811df72e210e20de99719539505da54762a11c6d", "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor", "authors": ["Tuomas Haarnoja", "Aurick Zhou", "Pieter Abbeel", "Sergey Levine"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "haarnoja18b.pdf", "abstract": "Model-free deep reinforcement learning (RL) al- gorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessi- tate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off- policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning frame- work. In this framework, the actor aims to maxi- mize expected reward while also maximizing en- tropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formu- lation, our method achieves state-of-the-art per- formance on a range of continuous control bench- mark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.", "values": ["novelty", "simplicity", "generalization", "robustness", "formal description/analysis", "theoretical guarantees", "approximation", "quantitative evidence (e.g. experiments)", "scientific methodology", "large scale", "promising", "automatic", "performance", "efficiency", "effectiveness", "successful", "unifying ideas or integrating components", "identifying limitations", "improvement", "used in practice/popular", "applies to real world", "building on past work"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "843959ffdccf31c6694d135fad07425924f785b1", "title": "Extracting and composing robust features with denoising autoencoders", "authors": ["Pascal Vincent", "Hugo Larochelle", "Yoshua Bengio", "Pierre-Antoine Manzagol"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "1390156.1390294.pdf", "abstract": "Previous work has shown that the diﬃcul- ties in learning deep generative or discrim- inative models can be overcome by an ini- tial unsupervised learning step that maps in- puts to useful intermediate representations. We introduce and motivate a new training principle for unsupervised learning of a rep- resentation based on the idea of making the learned representations robust to partial cor- ruption of the input pattern. This approach can be used to train autoencoders, and these denoising autoencoders can be stacked to ini- tialize deep architectures. The algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective. Comparative experiments clearly show the surprising ad- vantage of corrupting the input of autoen- coders on a pattern classiﬁcation benchmark suite.", "values": ["novelty", "simplicity", "generalization", "robustness", "theoretical guarantees", "quantitative evidence (e.g. experiments)", "scientific methodology", "performance", "efficiency", "successful", "unifying ideas or integrating components", "understanding (for researchers)", "learning from humans", "useful", "building on past work"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "8de174ab5419b9d3127695405efd079808e956e8", "title": "Curriculum learning", "authors": ["Yoshua Bengio", "Jérôme Louradour", "Jérôme Louradour", "Ronan Collobert", "Jason Weston"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "1553374.1553380.pdf", "abstract": "Humans and animals learn much better when the examples are not randomly presented but organized in a meaningful order which illus- trates gradually more concepts, and gradu- ally more complex ones. Here, we formal- ize such training strategies in the context of machine learning, and call them “curricu- lum learning”. In the context of recent re- search studying the diﬃculty of training in the presence of non-convex training criteria (for deep deterministic and stochastic neu- ral networks), we explore curriculum learn- ing in various set-ups. The experiments show that signiﬁcant improvements in generaliza- tion can be achieved. We hypothesize that curriculum learning has both an eﬀect on the speed of convergence of the training process to a minimum and, in the case of non-convex criteria, on the quality of the local minima obtained: curriculum learning can be seen as a particular form of continuation method (a general strategy for global optimization of non-convex functions).", "values": ["novelty", "simplicity", "generalization", "formal description/analysis", "quantitative evidence (e.g. experiments)", "scientific methodology", "human-like mechanism", "generality", "automatic", "performance", "efficiency", "effectiveness", "successful", "identifying limitations", "understanding (for researchers)", "improvement", "learning from humans", "useful", "deferral to humans", "beneficence", "building on past work"], "labels_source": "Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "9491bcc1e54b52bea617283f7f716cf009068bce", "title": "Clustered Multi-Task Learning: A Convex Formulation", "authors": ["Laurent Jacob", "Laurent Jacob", "Francis Bach", "Jean-Philippe Vert"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "NIPS-2008-clustered-multi-task-learning-a-convex-formulation-Paper.pdf", "abstract": "In multi-task learning several related tasks are considered simultaneously, with the hope that by an appropriate sharing of information across tasks, each task may beneﬁt from the others. In the context of learning linear functions for supervised classiﬁcation or regression, this can be achieved by including a priori informa- tion about the weight vectors associated with the tasks, and how they are expected to be related to each other. In this paper, we assume that tasks are clustered into groups, which are unknown beforehand, and that tasks within a group have similar weight vectors. We design a new spectral norm that encodes this a priori assump- tion, without the prior knowledge of the partition of tasks into groups, resulting in a new convex optimization formulation for multi-task learning. We show in simulations on synthetic examples and on the IEDB MHC-I binding dataset, that our approach outperforms well-known convex methods for multi-task learning, as well as related non-convex methods dedicated to the same problem.", "values": ["novelty", "generalization", "quantitative evidence (e.g. experiments)", "promising", "performance", "efficiency", "understanding (for researchers)", "practical", "useful", "building on past work"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "94be567c32ae76bdaadabd4975807a94181e39b3", "title": "How Does Batch Normalization Help Optimization?", "authors": ["Shibani Santurkar", "Dimitris Tsipras", "Andrew Ilyas", "Aleksander Mądry"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "NeurIPS-2018-how-does-batch-normalization-help-optimization-Paper.pdf", "abstract": "Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm’s effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers’ input distributions during training to reduce the so-called “internal covariate shift”. In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape signiﬁcantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training.", "values": ["generalization", "robustness", "formal description/analysis", "theoretical guarantees", "quantitative evidence (e.g. experiments)", "qualitative evidence (e.g. examples)", "scientific methodology", "controllability (of model owner)", "large scale", "generality", "exactness", "concreteness", "performance", "efficiency", "effectiveness", "successful", "unifying ideas or integrating components", "identifying limitations", "understanding (for researchers)", "improvement", "progress", "used in practice/popular", "applies to real world", "practical", "useful", "beneficence", "building on past work"], "labels_source": "Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "988a378f640eb7fb681f977d6cb1e0c830c07b4c", "title": "Adversarial Examples Are a Natural Consequence of Test Error in Noise", "authors": ["Nic Ford", "Justin Gilmer", "Nicolas Carlini", "Ekin Dogus Cubuk"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "gilmer19a.pdf", "abstract": "Over the last few years, the phenomenon of ad- versarial examples — maliciously constructed in- puts that fool trained machine learning models — has captured the attention of the research commu- nity, especially when the adversary is restricted to small modiﬁcations of a correctly handled in- put. Less surprisingly, image classiﬁers also lack human-level performance on randomly corrupted images, such as images with additive Gaussian noise. In this paper we provide both empirical and theoretical evidence that these are two manifes- tations of the same underlying phenomenon. We establish close connections between the adversar- ial robustness and corruption robustness research programs, with the strongest connection in the case of additive Gaussian noise. This suggests that improving adversarial robustness should go hand in hand with improving performance in the presence of more general and realistic image cor- ruptions. Based on our results we recommend that future adversarial defenses consider evaluating the robustness of their methods to distributional shift with benchmarks such as ImageNet-C.", "values": ["novelty", "generalization", "robustness", "realistic output", "theoretical guarantees", "quantitative evidence (e.g. experiments)", "scientific methodology", "human-like mechanism", "generality", "performance", "effectiveness", "successful", "unifying ideas or integrating components", "improvement", "parallelizability / distributed", "deferral to humans", "beneficence"], "labels_source": "Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "99990944b8516536f4ff2e5af6d9ea1c5993c0bd", "title": "Domain Adaptation with Multiple Sources", "authors": ["Yishay Mansour", "Yishay Mansour", "Mehryar Mohri", "Mehryar Mohri", "Afshin Rostamizadeh"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "NIPS-2008-domain-adaptation-with-multiple-sources-Paper.pdf", "abstract": "This paper presents a theoretical analysis of the problem of domain adaptation with multiple sources. For each source domain, the distribution over the input points as well as a hypothesis with error at most ǫ are given. The problem con- sists of combining these hypotheses to derive a hypothesis with small error with respect to the target domain. We present several theoretical results relating to this problem. In particular, we prove that standard convex combinations of the source hypotheses may in fact perform very poorly and that, instead, combinations weighted by the source distributions beneﬁt from favorable theoretical guarantees. Our main result shows that, remarkably, for any ﬁxed target function, there exists a distribution weighted combining rule that has a loss of at most ǫ with respect to any target mixture of the source distributions. We further generalize the setting from a single target function to multiple consistent target functions and show the existence of a combining rule with error at most 3ǫ. Finally, we report empirical results for a multiple source adaptation problem with a real-world dataset.", "values": ["novelty", "simplicity", "generalization", "formal description/analysis", "theoretical guarantees", "quantitative evidence (e.g. experiments)", "qualitative evidence (e.g. examples)", "scientific methodology", "large scale", "performance", "efficiency", "successful", "identifying limitations", "used in practice/popular", "applies to real world", "building on past work"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "9f9fc406c76255fec51a6196ce167c0ff1d1efc0", "title": "Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent", "authors": ["Jaehoon Lee", "Lechao Xiao", "Samuel S. Schoenholz", "Yasaman Bahri", "Roman Novak", "Jascha Sohl-Dickstein", "Jeffrey Pennington"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "NeurIPS-2019-wide-neural-networks-of-any-depth-evolve-as-linear-models-under-gradient-descent-Paper.pdf", "abstract": "A longstanding goal in deep learning research has been to precisely characterize training and generalization. However, the often complex loss landscapes of neural networks have made a theory of learning dynamics elusive. In this work, we show that for wide neural networks the learning dynamics simplify considerably and that, in the inﬁnite width limit, they are governed by a linear model obtained from the ﬁrst-order Taylor expansion of the network around its initial parameters. Fur- thermore, mirroring the correspondence between wide Bayesian neural networks and Gaussian processes, gradient-based training of wide neural networks with a squared loss produces test set predictions drawn from a Gaussian process with a particular compositional kernel. While these theoretical results are only exact in the inﬁnite width limit, we nevertheless ﬁnd excellent empirical agreement between the predictions of the original network and those of the linearized version even for ﬁnite practically-sized networks. This agreement is robust across different architectures, optimization methods, and loss functions.", "values": ["novelty", "simplicity", "generalization", "robustness", "formal description/analysis", "theoretical guarantees", "approximation", "quantitative evidence (e.g. experiments)", "scientific methodology", "large scale", "exactness", "preciseness", "performance", "unifying ideas or integrating components", "identifying limitations", "understanding (for researchers)", "facilitating use (e.g. sharing code)", "scales up", "applies to real world", "practical", "easy to work with", "building on past work"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "a188d2ac0d10bdd4d4a04c92cdc76523e11c155c", "title": "Privacy-preserving logistic regression", "authors": ["Kamalika Chaudhuri", "Claire Monteleoni"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "NIPS-2008-privacy-preserving-logistic-regression-Paper.pdf", "abstract": "This paper addresses the important tradeoff between privacy and learnability, when designing algorithms for learning from private databases. We focus on privacy-preserving logistic regression. First we apply an idea of Dwork et al. [6] to design a privacy-preserving logistic regression algorithm. This involves bound- ing the sensitivity of regularized logistic regression, and perturbing the learned classiﬁer with noise proportional to the sensitivity. We then provide a privacy-preserving regularized logistic regression algorithm based on a new privacy-preserving technique: solving a perturbed optimization problem. We prove that our algorithm preserves privacy in the model due to [6]. We provide learning guarantees for both algorithms, which are tighter for our new algorithm, in cases in which one would typically apply logistic regression. Ex- periments demonstrate improved learning performance of our method, versus the sensitivity method. Our privacy-preserving technique does not depend on the sen- sitivity of the function, and extends easily to a class of convex loss functions. Our work also reveals an interesting connection between regularization and privacy.", "values": ["novelty", "generalization", "formal description/analysis", "theoretical guarantees", "quantitative evidence (e.g. experiments)", "generality", "performance", "efficiency", "unifying ideas or integrating components", "understanding (for researchers)", "applies to real world", "privacy", "building on past work"], "labels_source": "Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "a22ac183c8b37824e32cae970db170b861a13438", "title": "Learning diverse rankings with multi-armed bandits", "authors": ["Filip Radlinski", "Robert Kleinberg", "Thorsten Joachims"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "Copy of 1390156.1390255.pdf", "abstract": "Algorithms for learning to rank Web docu- ments usually assume a document’s relevance is independent of other documents. This leads to learned ranking functions that pro- duce rankings with redundant results. In contrast, user studies have shown that di- versity at high ranks is often preferred. We present two online learning algorithms that directly learn a diverse ranking of documents based on users’ clicking behavior. We show that these algorithms minimize abandon- ment, or alternatively, maximize the proba- bility that a relevant document is found in the top k positions of a ranking. Moreover, one of our algorithms asymptotically achieves optimal worst-case performance even if users’ interests change.", "values": ["novelty", "generalization", "realistic output", "formal description/analysis", "theoretical guarantees", "approximation", "quantitative evidence (e.g. experiments)", "large scale", "preciseness", "performance", "efficiency", "identifying limitations", "improvement", "used in practice/popular", "user influence", "building on past work"], "labels_source": "Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "a53da9916b87fa295837617c16ef2ca6462cafb8", "title": "Classification using discriminative restricted Boltzmann machines", "authors": ["Hugo Larochelle", "Yoshua Bengio"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "Copy of 1390156.1390224.pdf", "abstract": "Recently, many applications for Restricted Boltzmann Machines (RBMs) have been de- veloped for a large variety of learning prob- lems. However, RBMs are usually used as feature extractors for another learning al- gorithm or to provide a good initialization for deep feed-forward neural network clas- siﬁers, and are not considered as a stand- alone solution to classiﬁcation problems. In this paper, we argue that RBMs provide a self-contained framework for deriving com- petitive non-linear classiﬁers. We present an evaluation of diﬀerent learning algorithms for RBMs which aim at introducing a discrimi- native component to RBM training and im- prove their performance as classiﬁers. This approach is simple in that RBMs are used directly to build a classiﬁer, rather than as a stepping stone. Finally, we demonstrate how discriminative RBMs can also be successfully employed in a semi-supervised setting.", "values": ["novelty", "simplicity", "generalization", "theoretical guarantees", "approximation", "quantitative evidence (e.g. experiments)", "performance", "efficiency", "successful", "scales up", "applies to real world", "useful", "building on past work"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "a8f3dc53e321fbb2565f5925def4365b9f68d1af", "title": "Self-Attention Generative Adversarial Networks", "authors": ["Han Zhang", "Han Zhang", "Ian J. Goodfellow", "Dimitris N. Metaxas", "Augustus Odena"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "zhang19d.pdf", "abstract": "In this paper, we propose the Self-Attention Gen- erative Adversarial Network (SAGAN) which allows attention-driven, long-range dependency modeling for image generation tasks. Traditional convolutional GANs generate high-resolution de- tails as a function of only spatially local points in lower-resolution feature maps. In SAGAN, de- tails can be generated using cues from all feature locations. Moreover, the discriminator can check that highly detailed features in distant portions of the image are consistent with each other. Fur- thermore, recent work has shown that generator conditioning affects GAN performance. Leverag- ing this insight, we apply spectral normalization to the GAN generator and ﬁnd that this improves training dynamics. The proposed SAGAN per- forms better than prior work1, boosting the best published Inception score from 36.8 to 52.52 and reducing Fr´echet Inception distance from 27.62 to 18.65 on the challenging ImageNet dataset. Visu- alization of the attention layers shows that the gen- erator leverages neighborhoods that correspond to object shapes rather than local regions of ﬁxed shape.", "values": ["generalization", "robustness", "realistic output", "quantitative evidence (e.g. experiments)", "performance", "efficiency", "effectiveness", "successful", "unifying ideas or integrating components", "identifying limitations", "understanding (for researchers)", "progress", "facilitating use (e.g. sharing code)", "building on past work"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "a9022d8ffb5e417458fba9a280f90c1b08cb6c73", "title": "Stronger generalization bounds for deep nets via a compression approach", "authors": ["Sanjeev Arora", "Rong Ge", "Behnam Neyshabur", "Yi Zhang"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "Copy of arora18b.pdf", "abstract": "Deep nets generalize well despite having more parameters than the number of training samples. Recent works try to give an explanation using PAC-Bayes and Margin-based analyses, but do not as yet result in sample complexity bounds better than naive parameter counting. The cur- rent paper shows generalization bounds that are orders of magnitude better in practice. These rely upon new succinct reparametrizations of the trained net — a compression that is explicit and efﬁcient. These yield generalization bounds via a simple compression-based framework introduced here. Our results also provide some theoretical justiﬁcation for widespread empirical success in compressing deep nets. Analysis of correctness of our compression relies upon some newly iden- tiﬁed “noise stability”properties of trained deep nets, which are also experimentally veriﬁed. The study of these properties and resulting general- ization bounds are also extended to convolutional nets, which had eluded earlier attempts on proving generalization.", "values": ["simplicity", "generalization", "robustness", "formal description/analysis", "theoretical guarantees", "quantitative evidence (e.g. experiments)", "qualitative evidence (e.g. examples)", "performance", "efficiency", "understanding (for researchers)", "applies to real world", "practical", "building on past work"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "a9ba0f45b97b3c5c261efb52d5e33490b60cbe10", "title": "Translated Learning: Transfer Learning across Different Feature Spaces", "authors": ["Wenyuan Dai", "Yuqiang Chen", "Gui-Rong Xue", "Qiang Yang", "Yong Yu"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "Copy of NIPS-2008-translated-learning-transfer-learning-across-different-feature-spaces-Paper.pdf", "abstract": "This paper investigates a new machine learning strategy called translated learn- ing. Unlike many previous learning tasks, we focus on how to use labeled data from one feature space to enhance the classiﬁcation of other entirely different learning spaces. For example, we might wish to use labeled text data to help learn a model for classifying image data, when the labeled images are difﬁcult to ob- tain. An important aspect of translated learning is to build a “bridge” to link one feature space (known as the “source space”) to another space (known as the “tar- get space”) through a translator in order to migrate the knowledge from source to target. The translated learning solution uses a language model to link the class labels to the features in the source spaces, which in turn is translated to the fea- tures in the target spaces. Finally, this chain of linkages is completed by tracing back to the instances in the target spaces. We show that this path of linkage can be modeled using a Markov chain and risk minimization. Through experiments on the text-aided image classiﬁcation and cross-language classiﬁcation tasks, we demonstrate that our translated learning framework can greatly outperform many state-of-the-art baseline methods.", "values": ["novelty", "simplicity", "generalization", "formal description/analysis", "approximation", "quantitative evidence (e.g. experiments)", "qualitative evidence (e.g. examples)", "large scale", "generality", "performance", "efficiency", "effectiveness", "successful", "unifying ideas or integrating components", "facilitating use (e.g. sharing code)", "building on past work"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "aa5741c74b7fac10680c1cfbdd49d9ffb5751a68", "title": "Using Pre-Training Can Improve Model Robustness and Uncertainty", "authors": ["Dan Hendrycks", "Kimin Lee", "Mantas Mazeika"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "Copy of hendrycks19a.pdf", "abstract": "He et al. (2018) have called into question the utility of pre-training by showing that train- ing from scratch can often yield similar per- formance to pre-training. We show that al- though pre-training may not improve perfor- mance on traditional classiﬁcation metrics, it im- proves model robustness and uncertainty esti- mates. Through extensive experiments on la- bel corruption, class imbalance, adversarial ex- amples, out-of-distribution detection, and con- ﬁdence calibration, we demonstrate large gains from pre-training and complementary effects with task-speciﬁc methods. We show approx- imately a 10% absolute improvement over the previous state-of-the-art in adversarial robust- ness. In some cases, using pre-training without task-speciﬁc methods also surpasses the state- of-the-art, highlighting the need for pre-training when evaluating future methods on robustness and uncertainty tasks.", "values": ["generalization", "robustness", "quantitative evidence (e.g. experiments)", "performance", "efficiency", "understanding (for researchers)", "improvement", "used in practice/popular", "applies to real world", "building on past work"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "ad7129af0644dbcafa9aa2f111cb76526ea444a1", "title": "Defending Against Neural Fake News", "authors": ["Rowan Zellers", "Ari Holtzman", "Hannah Rashkin", "Yonatan Bisk", "Ali Farhadi", "Ali Farhadi", "Franziska Roesner", "Yejin Choi", "Yejin Choi"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "Copy of NeurIPS-2019-defending-against-neural-fake-news-Paper.pdf", "abstract": "Recent progress in natural language generation has raised dual-use concerns. While applications like summarization and translation are positive, the underlying tech- nology also might enable adversaries to generate neural fake news: targeted propa- ganda that closely mimics the style of real news. Modern computer security relies on careful threat modeling: identifying potential threats and vulnerabilities from an adversary’s point of view, and exploring potential mitigations to these threats. Likewise, developing robust defenses against neural fake news requires us ﬁrst to carefully investigate and characterize the risks of these models. We thus present a model for controllable text generation called Grover. Given a headline like ‘Link Found Between Vaccines and Autism,’ Grover can generate the rest of the article; humans ﬁnd these generations to be more trustworthy than human-written disinformation. Developing robust veriﬁcation techniques against generators like Grover is critical. We ﬁnd that best current discriminators can classify neural fake news from real, human-written, news with 73% accuracy, assuming access to a moderate level of training data. Counterintuitively, the best defense against Grover turns out to be Grover itself, with 92% accuracy, demonstrating the importance of public release of strong generators. We investigate these results further, showing that exposure bias – and sampling strategies that alleviate its e↵ects – both leave artifacts that similar discriminators can pick up on. We conclude by discussing ethical issues regarding the technology, and plan to release Grover publicly, helping pave the way for better detection of neural fake news.", "values": ["novelty", "generalization", "robustness", "realistic output", "quantitative evidence (e.g. experiments)", "qualitative evidence (e.g. examples)", "scientific methodology", "controllability (of model owner)", "human-like mechanism", "large scale", "concreteness", "performance", "efficiency", "effectiveness", "unifying ideas or integrating components", "identifying limitations", "critique", "understanding (for researchers)", "progress", "easy to implement", "requires few resources", "parallelizability / distributed", "facilitating use (e.g. sharing code)", "scales up", "applies to real world", "learning from humans", "transparent (to users)", "fairness", "not socially biased", "collective influence", "deferral to humans", "beneficence", "non-maleficence", "realistic world model", "building on past work"], "labels_source": "Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "ae6e206c8c2994e04c3fdc5bd97d81fdd0f27493", "title": "On the Complexity of Linear Prediction: Risk Bounds, Margin Bounds, and Regularization", "authors": ["Sham M Kakade", "Karthik Sridharan", "Ambuj Tewari"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "Copy of NIPS-2008-on-the-complexity-of-linear-prediction-risk-bounds-margin-bounds-and-regularization-Paper.pdf", "abstract": "This work characterizes the generalization ability of algorithms whose predic- tions are linear in the input vector. To this end, we provide sharp bounds for Rademacher and Gaussian complexities of (constrained) linear classes, which di- rectly lead to a number of generalization bounds. This derivation provides simpli- ﬁed proofs of a number of corollaries including: risk bounds for linear prediction (including settings where the weight vectors are constrained by either L2 or L1 constraints), margin bounds (including both L2 and L1 margins, along with more general notions based on relative entropy), a proof of the PAC-Bayes theorem, and upper bounds on L2 covering numbers (with Lp norm constraints and rela- tive entropy constraints). In addition to providing a uniﬁed analysis, the results herein provide some of the sharpest risk and margin bounds. Interestingly, our results show that the uniform convergence rates of empirical risk minimization algorithms tightly match the regret bounds of online learning algorithms for linear prediction, up to a constant factor of 2.", "values": ["novelty", "simplicity", "generalization", "formal description/analysis", "theoretical guarantees", "quantitative evidence (e.g. experiments)", "qualitative evidence (e.g. examples)", "scientific methodology", "generality", "preciseness", "performance", "unifying ideas or integrating components", "understanding (for researchers)", "parallelizability / distributed", "applies to real world", "building on past work"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "b32de117302258dd29919435cd001a8bcdfee3b3", "title": "Replicated Softmax: an Undirected Topic Model", "authors": ["Geoffrey E. Hinton", "Ruslan R Salakhutdinov"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "NIPS-2009-replicated-softmax-an-undirected-topic-model-Paper.pdf", "abstract": "We introduce a two-layer undirected graphical model, called a “Replicated Soft- max”, that can be used to model and automatically extract low-dimensional latent semantic representations from a large unstructured collection of documents. We present efﬁcient learning and inference algorithms for this model, and show how a Monte-Carlo based method, Annealed Importance Sampling, can be used to pro- duce an accurate estimate of the log-probability the model assigns to test data. This allows us to demonstrate that the proposed model is able to generalize much better compared to Latent Dirichlet Allocation in terms of both the log-probability of held-out documents and the retrieval accuracy.", "values": ["novelty", "simplicity", "generalization", "realistic output", "formal description/analysis", "approximation", "quantitative evidence (e.g. experiments)", "large scale", "generality", "exactness", "preciseness", "automatic", "performance", "efficiency", "identifying limitations", "used in practice/popular", "scales up", "interpretable (to users)", "easy to work with", "building on past work"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "b3f1aa12dde233aaf543bb9ccb27213c494e0fd5", "title": "Unlabeled Data Improves Adversarial Robustness", "authors": ["Yair Carmon", "Aditi Raghunathan", "Ludwig Schmidt", "John C. Duchi", "Percy S. Liang"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "Copy of NeurIPS-2019-unlabeled-data-improves-adversarial-robustness-Paper.pdf", "abstract": "We demonstrate, theoretically and empirically, that adversarial robustness can signiﬁcantly beneﬁt from semisupervised learning. Theoretically, we revisit the simple Gaussian model of Schmidt et al. [41] that shows a sample complexity gap between standard and robust classiﬁcation. We prove that unlabeled data bridges this gap: a simple semisupervised learning procedure (self-training) achieves high robust accuracy using the same number of labels required for achieving high stan- dard accuracy. Empirically, we augment CIFAR-10 with 500K unlabeled images sourced from 80 Million Tiny Images and use robust self-training to outperform state-of-the-art robust accuracies by over 5 points in (i) `1 robustness against sev- eral strong attacks via adversarial training and (ii) certiﬁed `2 and `1 robustness via randomized smoothing. On SVHN, adding the dataset’s own extra training set with the labels removed provides gains of 4 to 10 points, within 1 point of the gain from using the extra labels.", "values": ["generalization", "robustness", "formal description/analysis", "theoretical guarantees", "quantitative evidence (e.g. experiments)", "scientific methodology", "large scale", "generality", "performance", "efficiency", "effectiveness", "understanding (for researchers)", "scales up", "practical", "useful", "building on past work"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "b3f83e8416010e9c3a705a0b6390d268e5ddf5c0", "title": "Black-box Adversarial Attacks with Limited Queries and Information", "authors": ["Andrew Ilyas", "Andrew Ilyas", "Logan Engstrom", "Logan Engstrom", "Anish Athalye", "Anish Athalye", "Jessy Lin", "Jessy Lin"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "Copy of ilyas18a.pdf", "abstract": "Current neural network-based classiﬁers are sus- ceptible to adversarial examples even in the black-box setting, where the attacker only has query access to the model. In practice, the threat model for real-world systems is often more re- strictive than the typical black-box model where the adversary can observe the full output of the network on arbitrarily many chosen inputs. We deﬁne three realistic threat models that more accurately characterize many real-world clas- siﬁers: the query-limited setting, the partial- information setting, and the label-only setting. We develop new attacks that fool classiﬁers un- der these more restrictive threat models, where previous methods would be impractical or inef- fective. We demonstrate that our methods are ef- fective against an ImageNet classiﬁer under our proposed threat models. We also demonstrate a targeted black-box attack against a commercial classiﬁer, overcoming the challenges of limited query access, partial information, and other prac- tical issues to break the Google Cloud Vision API.", "values": ["novelty", "generalization", "robustness", "formal description/analysis", "theoretical guarantees", "quantitative evidence (e.g. experiments)", "qualitative evidence (e.g. examples)", "large scale", "performance", "efficiency", "effectiveness", "successful", "unifying ideas or integrating components", "identifying limitations", "requires few resources", "scales up", "applies to real world", "practical", "security", "realistic world model", "building on past work"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "c42816f497d663c681df20d48a6e66a5632600d8", "title": "MixMatch: A Holistic Approach to Semi-Supervised Learning", "authors": ["David Berthelot", "Nicholas Carlini", "Ian Goodfellow", "Nicolas Papernot", "Avital Oliver", "Colin A. Raffel"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "NeurIPS-2019-mixmatch-a-holistic-approach-to-semi-supervised-learning-Paper.pdf", "abstract": "Semi-supervised learning has proven to be a powerful paradigm for leveraging unlabeled data to mitigate the reliance on large labeled datasets. In this work, we unify the current dominant approaches for semi-supervised learning to produce a new algorithm, MixMatch, that guesses low-entropy labels for data-augmented un- labeled examples and mixes labeled and unlabeled data using MixUp. MixMatch obtains state-of-the-art results by a large margin across many datasets and labeled data amounts. For example, on CIFAR-10 with 250 labels, we reduce error rate by a factor of 4 (from 38% to 11%) and by a factor of 2 on STL-10. We also demonstrate how MixMatch can help achieve a dramatically better accuracy-privacy trade-off for differential privacy. Finally, we perform an ablation study to tease apart which components of MixMatch are most important for its success. We release all code used in our experiments.1", "values": ["novelty", "simplicity", "generalization", "quantitative evidence (e.g. experiments)", "qualitative evidence (e.g. examples)", "large scale", "performance", "efficiency", "effectiveness", "successful", "unifying ideas or integrating components", "understanding (for researchers)", "improvement", "facilitating use (e.g. sharing code)", "useful", "privacy", "deferral to humans", "building on past work"], "labels_source": "Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "c92be891c5f8f0f60b6de206364f9a744612d1e8", "title": "Adversarial Training for Free!", "authors": ["Ali Shafahi", "Mahyar Najibi", "Mohammad Amin Ghiasi", "Zheng Xu", "John Dickerson", "Christoph Studer", "Larry S. Davis", "Gavin Taylor", "Tom Goldstein"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "Copy of NeurIPS-2019-adversarial-training-for-free-Paper.pdf", "abstract": "Adversarial training, in which a network is trained on adversarial examples, is one of the few defenses against adversarial attacks that withstands strong attacks. Un- fortunately, the high cost of generating strong adversarial examples makes standard adversarial training impractical on large-scale problems like ImageNet. We present an algorithm that eliminates the overhead cost of generating adversarial examples by recycling the gradient information computed when updating model parameters. Our “free” adversarial training algorithm achieves comparable robustness to PGD adversarial training on the CIFAR-10 and CIFAR-100 datasets at negligible addi- tional cost compared to natural training, and can be 7 to 30 times faster than other strong adversarial training methods. Using a single workstation with 4 P100 GPUs and 2 days of runtime, we can train a robust model for the large-scale ImageNet classiﬁcation task that maintains 40% accuracy against PGD attacks.", "values": ["novelty", "simplicity", "generalization", "robustness", "performance", "efficiency", "requires few resources", "parallelizability / distributed", "scales up", "interpretable (to users)", "building on past work"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "cc97aae9c1bc35e01a737aa4bf5fa4677505ec44", "title": "Deflation Methods for Sparse PCA", "authors": ["Lester W. Mackey"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "NIPS-2008-deflation-methods-for-sparse-pca-Paper.pdf", "abstract": "In analogy to the PCA setting, the sparse PCA problem is often solved by iter- atively alternating between two subtasks: cardinality-constrained rank-one vari- ance maximization and matrix deﬂation. While the former has received a great deal of attention in the literature, the latter is seldom analyzed and is typically borrowed without justiﬁcation from the PCA context. In this work, we demon- strate that the standard PCA deﬂation procedure is seldom appropriate for the sparse PCA setting. To rectify the situation, we ﬁrst develop several deﬂation al- ternatives better suited to the cardinality-constrained context. We then reformulate the sparse PCA optimization problem to explicitly reﬂect the maximum additional variance objective on each round. The result is a generalized deﬂation procedure that typically outperforms more standard techniques on real-world datasets.", "values": ["novelty", "generalization", "formal description/analysis", "theoretical guarantees", "approximation", "quantitative evidence (e.g. experiments)", "scientific methodology", "generality", "performance", "efficiency", "identifying limitations", "improvement", "used in practice/popular", "applies to real world", "interpretable (to users)", "building on past work"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "ccb1bafdae68c635cbd30d49fda7dbf88a3ce1b6", "title": "Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data", "authors": ["Yuanzhi Li", "Yingyu Liang"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "Copy of NeurIPS-2018-learning-overparameterized-neural-networks-via-stochastic-gradient-descent-on-structured-data-Paper.pdf", "abstract": "Neural networks have many successful applications, while much less theoretical understanding has been gained. Towards bridging this gap, we study the problem of learning a two-layer overparameterized ReLU neural network for multi-class clas- siﬁcation via stochastic gradient descent (SGD) from random initialization. In the overparameterized setting, when the data comes from mixtures of well-separated distributions, we prove that SGD learns a network with a small generalization error, albeit the network has enough capacity to ﬁt arbitrary labels. Furthermore, the analysis provides interesting insights into several aspects of learning neural networks and can be veriﬁed based on empirical studies on synthetic data and on the MNIST dataset.", "values": ["generalization", "formal description/analysis", "theoretical guarantees", "approximation", "quantitative evidence (e.g. experiments)", "performance", "understanding (for researchers)", "applies to real world", "practical", "building on past work"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "cd10a147c94ecba92c8a400aa0fd04f912b4900c", "title": "Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning", "authors": ["Francis R. Bach"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "Copy of NIPS-2008-exploring-large-feature-spaces-with-hierarchical-multiple-kernel-learning-Paper.pdf", "abstract": "For supervised and unsupervised learning, positive deﬁnite kernels allow to use large and potentially inﬁnite dimensional feature spaces with a computational cost that only depends on the number of observations. This is usually done through the penalization of predictor functions by Euclidean or Hilbertian norms. In this paper, we explore penalizing by sparsity-inducing norms such as the ℓ1-norm or the block ℓ1-norm. We assume that the kernel decomposes into a large sum of individual basis kernels which can be embedded in a directed acyclic graph; we show that it is then possible to perform kernel selection through a hierarchical multiple kernel learning framework, in polynomial time in the number of selected kernels. This framework is naturally applied to non linear variable selection; our extensive simulations on synthetic datasets and datasets from the UCI repository show that efﬁciently exploring the large feature space through sparsity-inducing norms leads to state-of-the-art predictive performance.", "values": ["novelty", "generalization", "quantitative evidence (e.g. experiments)", "qualitative evidence (e.g. examples)", "scientific methodology", "large scale", "exactness", "preciseness", "performance", "efficiency", "unifying ideas or integrating components", "building on past work"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "ceb3eff13951d80813103172355820bc9493f172", "title": "Learning with structured sparsity", "authors": ["Junzhou Huang", "Tong Zhang", "Dimitris Metaxas"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "Copy of 1553374.1553429.pdf", "abstract": "This paper investigates a new learning formula- tion called structured sparsity, which is a natu- ral extension of the standard sparsity concept in statistical learning and compressive sensing. By allowing arbitrary structures on the feature set, this concept generalizes the group sparsity idea. A general theory is developed for learning with structured sparsity, based on the notion of coding complexity associated with the structure. More- over, a structured greedy algorithm is proposed to efﬁciently solve the structured sparsity prob- lem. Experiments demonstrate the advantage of structured sparsity over standard sparsity.", "values": ["novelty", "generalization", "formal description/analysis", "theoretical guarantees", "approximation", "quantitative evidence (e.g. experiments)", "qualitative evidence (e.g. examples)", "performance", "efficiency", "effectiveness", "unifying ideas or integrating components", "identifying limitations", "used in practice/popular", "applies to real world", "building on past work"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "cf40878c2de992b6be053f79d3e97d20307dba26", "title": "Multi-Label Prediction via Compressed Sensing", "authors": ["Daniel Hsu", "Sham M. Kakade", "John Langford", "Tong Zhang"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "Copy of NIPS-2009-multi-label-prediction-via-compressed-sensing-Paper.pdf", "abstract": "We consider multi-label prediction problems with large output spaces under the assumption of output sparsity – that the target (label) vectors have small support. We develop a general theory for a variant of the popular error correcting output code scheme, using ideas from compressed sensing for exploiting this sparsity. The method can be regarded as a simple reduction from multi-label regression problems to binary regression problems. We show that the number of subprob- lems need only be logarithmic in the total number of possible labels, making this approach radically more efﬁcient than others. We also state and prove robustness guarantees for this method in the form of regret transform bounds (in general), and also provide a more detailed analysis for the linear prediction setting.", "values": ["novelty", "simplicity", "generalization", "robustness", "formal description/analysis", "theoretical guarantees", "approximation", "quantitative evidence (e.g. experiments)", "scientific methodology", "large scale", "generality", "automatic", "performance", "efficiency", "effectiveness", "unifying ideas or integrating components", "used in practice/popular", "scales up", "useful", "building on past work"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "cf80cc34528273d8fbe17783efe802a6509e1562", "title": "Online dictionary learning for sparse coding", "authors": ["Julien Mairal", "Francis Bach", "Jean Ponce", "Guillermo Sapiro"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "Copy of 1553374.1553463.pdf", "abstract": "Sparse coding—that is, modelling data vectors as sparse linear combinations of basis elements—is widely used in machine learning, neuroscience, signal processing, and statistics. This paper fo- cuses on learning the basis set, also called dic- tionary, to adapt it to speciﬁc data, an approach that has recently proven to be very effective for signal reconstruction and classiﬁcation in the au- dio and image processing domains. This paper proposes a new online optimization algorithm for dictionary learning, based on stochastic ap- proximations, which scales up gracefully to large datasets with millions of training samples. A proof of convergence is presented, along with experiments with natural images demonstrating that it leads to faster performance and better dic- tionaries than classical batch algorithms for both small and large datasets.", "values": ["novelty", "generalization", "approximation", "quantitative evidence (e.g. experiments)", "qualitative evidence (e.g. examples)", "promising", "concreteness", "performance", "efficiency", "effectiveness", "improvement", "scales up"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "d18b48f77eb5c517a6d2c1fa434d2952a1b0a825", "title": "Hierarchical Graph Representation Learning with Differentiable Pooling", "authors": ["Rex Ying", "Jiaxuan You", "Christopher Morris", "Xiang Ren", "William L. Hamilton", "Jure Leskovec"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "NeurIPS-2018-hierarchical-graph-representation-learning-with-differentiable-pooling-Paper.pdf", "abstract": "Recently, graph neural networks (GNNs) have revolutionized the ﬁeld of graph representation learning through effectively learned node embeddings, and achieved state-of-the-art results in tasks such as node classiﬁcation and link prediction. However, current GNN methods are inherently ﬂat and do not learn hierarchical representations of graphs—a limitation that is especially problematic for the task of graph classiﬁcation, where the goal is to predict the label associated with an entire graph. Here we propose DIFFPOOL, a differentiable graph pooling module that can generate hierarchical representations of graphs and can be combined with various graph neural network architectures in an end-to-end fashion. DIFFPOOL learns a differentiable soft cluster assignment for nodes at each layer of a deep GNN, mapping nodes to a set of clusters, which then form the coarsened input for the next GNN layer. Our experimental results show that combining existing GNN methods with DIFFPOOL yields an average improvement of 5–10% accuracy on graph classiﬁcation benchmarks, compared to all existing pooling approaches, achieving a new state-of-the-art on four out of ﬁve benchmark data sets.", "values": ["novelty", "simplicity", "generalization", "quantitative evidence (e.g. experiments)", "generality", "performance", "efficiency", "effectiveness", "successful", "unifying ideas or integrating components", "identifying limitations", "applies to real world", "interpretable (to users)", "building on past work"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "d9f6ada77448664b71128bb19df15765336974a6", "title": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems", "authors": ["Alex Wang", "Yada Pruksachatkun", "Nikita Nangia", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel R. Bowman"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "Copy of NeurIPS-2019-superglue-a-stickier-benchmark-for-general-purpose-language-understanding-systems-Paper.pdf", "abstract": "In the last year, new models and methods for pretraining and transfer learning have driven striking performance improvements across a range of language understand- ing tasks. The GLUE benchmark, introduced a little over one year ago, offers a single-number metric that summarizes progress on a diverse set of such tasks, but performance on the benchmark has recently surpassed the level of non-expert humans, suggesting limited headroom for further research. In this paper we present SuperGLUE, a new benchmark styled after GLUE with a new set of more difﬁ- cult language understanding tasks, a software toolkit, and a public leaderboard. SuperGLUE is available at super.gluebenchmark.com.", "values": ["novelty", "simplicity", "generalization", "robustness", "quantitative evidence (e.g. experiments)", "qualitative evidence (e.g. examples)", "human-like mechanism", "generality", "performance", "efficiency", "effectiveness", "unifying ideas or integrating components", "identifying limitations", "understanding (for researchers)", "improvement", "progress", "facilitating use (e.g. sharing code)", "learning from humans", "deferral to humans", "beneficence", "justice", "building on past work"], "labels_source": "Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "daf8cd0f2c159d022477914bfacee9ff6da70c8b", "title": "Adversarial Training and Robustness for Multiple Perturbations", "authors": ["Florian Tramer", "Dan Boneh"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "NeurIPS-2019-adversarial-training-and-robustness-for-multiple-perturbations-Paper.pdf", "abstract": "Defenses against adversarial examples, such as adversarial training, are typically tailored to a single perturbation type (e.g., small ℓ∞-noise). For other perturbations, these defenses offer no guarantees and, at times, even increase the model’s vulnera- bility. Our aim is to understand the reasons underlying this robustness trade-off, and to train models that are simultaneously robust to multiple perturbation types. We prove that a trade-off in robustness to different types of ℓp-bounded and spatial perturbations must exist in a natural and simple statistical setting. We corroborate our formal analysis by demonstrating similar robustness trade-offs on MNIST and CIFAR10. We propose new multi-perturbation adversarial training schemes, as well as an efﬁcient attack for the ℓ1-norm, and use these to show that models trained against multiple attacks fail to achieve robustness competitive with that of models trained on each attack individually. In particular, we ﬁnd that adversarial training with ﬁrst-order ℓ∞, ℓ1 and ℓ2 attacks on MNIST achieves merely 50% robust accuracy, partly because of gradient-masking. Finally, we propose afﬁne attacks that linearly interpolate between perturbation types and further degrade the accuracy of adversarially trained models.", "values": ["novelty", "simplicity", "generalization", "robustness", "formal description/analysis", "theoretical guarantees", "quantitative evidence (e.g. experiments)", "performance", "efficiency", "identifying limitations", "understanding (for researchers)", "facilitating use (e.g. sharing code)", "scales up", "applies to real world", "non-maleficence", "respect for law and public interest", "security", "building on past work"], "labels_source": "Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "e0c6abdbdecf04ffac65c440da77fb9d66bb474c", "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "authors": ["Zhilin Yang", "Zihang Dai", "Zihang Dai", "Yiming Yang", "Jaime G. Carbonell", "Ruslan Salakhutdinov", "Quoc V. Le"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "NeurIPS-2019-xlnet-generalized-autoregressive-pretraining-for-language-understanding-Paper.pdf", "abstract": "With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining ap- proaches based on autoregressive language modeling. However, relying on corrupt- ing the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-ﬁnetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment setting, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.1.", "values": ["novelty", "generalization", "formal description/analysis", "approximation", "quantitative evidence (e.g. experiments)", "scientific methodology", "large scale", "exactness", "performance", "efficiency", "effectiveness", "successful", "unifying ideas or integrating components", "understanding (for researchers)", "improvement", "parallelizability / distributed", "applies to real world", "beneficence", "easy to work with", "building on past work"], "labels_source": "Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "e337c5e4c23999c36f64bcb33ebe6b284e1bcbf1", "title": "Large-scale deep unsupervised learning using graphics processors", "authors": ["Rajat Raina", "Anand Madhavan", "Andrew Y. Ng"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "Copy of 1553374.1553486.pdf", "abstract": "The promise of unsupervised learning meth- ods lies in their potential to use vast amounts of unlabeled data to learn complex, highly nonlinear models with millions of free param- eters. We consider two well-known unsuper- vised learning models, deep belief networks (DBNs) and sparse coding, that have recently been applied to a ﬂurry of machine learning applications (Hinton & Salakhutdinov, 2006; Raina et al., 2007). Unfortunately, current learning algorithms for both models are too slow for large-scale applications, forcing re- searchers to focus on smaller-scale models, or to use fewer training examples. In this paper, we suggest massively paral- lel methods to help resolve these problems. We argue that modern graphics processors far surpass the computational capabilities of multicore CPUs, and have the potential to revolutionize the applicability of deep unsu- pervised learning methods. We develop gen- eral principles for massively parallelizing un- supervised learning tasks using graphics pro- cessors. We show that these principles can be applied to successfully scaling up learning algorithms for both DBNs and sparse coding. Our implementation of DBN learning is up to 70 times faster than a dual-core CPU imple- mentation for large models. For example, we are able to reduce the time required to learn a four-layer DBN with 100 million free param- eters from several weeks to around a single day. For sparse coding, we develop a simple, inherently parallel algorithm, that leads to a 5 to 15-fold speedup over previous methods. Appearing in Proceedings of the 26 th International Confer- ence on Machine Learning, Montreal, Canada, 2009. Copy- right 2009 by the author(s)/owner(s).", "values": ["novelty", "simplicity", "qualitative evidence (e.g. examples)", "large scale", "promising", "generality", "performance", "efficiency", "successful", "unifying ideas or integrating components", "identifying limitations", "used in practice/popular", "easy to implement", "requires few resources", "parallelizability / distributed", "facilitating use (e.g. sharing code)", "scales up", "applies to real world", "building on past work"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "e5554c9d5fa92af69992d72ed1fdfbe953b03fb4", "title": "Rethinking LDA: Why Priors Matter", "authors": ["David M. Mimno", "Andrew Mccallum"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "Copy of NIPS-2009-rethinking-lda-why-priors-matter-Paper (1).pdf", "abstract": "Implementations of topic models typically use symmetric Dirichlet priors with ﬁxed concentration parameters, with the implicit assumption that such “smoothing parameters” have little practical effect. In this paper, we explore several classes of structured priors for topic models. We ﬁnd that an asymmetric Dirichlet prior over the document–topic distributions has substantial advantages over a symmet- ric prior, while an asymmetric prior over the topic–word distributions provides no real beneﬁt. Approximation of this prior structure through simple, efﬁcient hy- perparameter optimization steps is sufﬁcient to achieve these performance gains. The prior structure we advocate substantially increases the robustness of topic models to variations in the number of topics and to the highly skewed word fre- quency distributions common in natural language. Since this prior structure can be implemented using efﬁcient algorithms that add negligible cost beyond standard inference techniques, we recommend it as a new standard for topic modeling.", "values": ["simplicity", "robustness", "approximation", "quantitative evidence (e.g. experiments)", "qualitative evidence (e.g. examples)", "scientific methodology", "large scale", "performance", "efficiency", "unifying ideas or integrating components", "identifying limitations", "understanding (for researchers)", "used in practice/popular", "easy to implement", "requires few resources", "applies to real world", "useful", "building on past work"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "ebd660b8df9d7f9cd044c63778c15c39cdd072ed", "title": "Guaranteed Rank Minimization via Singular Value Projection", "authors": ["Prateek Jain", "Raghu Meka", "Inderjit S. Dhillon"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "Copy of NIPS-2010-guaranteed-rank-minimization-via-singular-value-projection-Paper.pdf", "abstract": "Minimizing the rank of a matrix subject to afﬁne constraints is a fundamental problem with many important applications in machine learning and statistics. In this paper we propose a simple and fast algorithm SVP (Singular Value Projec- tion) for rank minimization under afﬁne constraints (ARMP) and show that SVP recovers the minimum rank solution for afﬁne constraints that satisfy a restricted isometry property (RIP). Our method guarantees geometric convergence rate even in the presence of noise and requires strictly weaker assumptions on the RIP con- stants than the existing methods. We also introduce a Newton-step for our SVP framework to speed-up the convergence with substantial empirical gains. Next, we address a practically important application of ARMP - the problem of low- rank matrix completion, for which the deﬁning afﬁne constraints do not directly obey RIP, hence the guarantees of SVP do not hold. However, we provide partial progress towards a proof of exact recovery for our algorithm by showing a more restricted isometry property and observe empirically that our algorithm recovers low-rank incoherent matrices from an almost optimal number of uniformly sam- pled entries. We also demonstrate empirically that our algorithms outperform ex- isting methods, such as those of [5, 18, 14], for ARMP and the matrix completion problem by an order of magnitude and are also more robust to noise and sampling schemes. In particular, results show that our SVP-Newton method is signiﬁcantly robust to noise and performs impressively on a more realistic power-law sampling scheme for the matrix completion problem.", "values": ["novelty", "simplicity", "generalization", "robustness", "formal description/analysis", "theoretical guarantees", "approximation", "quantitative evidence (e.g. experiments)", "generality", "exactness", "preciseness", "performance", "efficiency", "used in practice/popular", "applies to real world", "learning from humans", "practical", "useful", "building on past work"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc", "title": "Cross-lingual Language Model Pretraining", "authors": ["Guillaume Lample", "Guillaume Lample", "Alexis Conneau", "Alexis Conneau"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "Copy of NeurIPS-2019-cross-lingual-language-model-pretraining-Paper.pdf", "abstract": "Recent studies have demonstrated the efﬁciency of generative pretraining for En- glish natural language understanding. In this work, we extend this approach to multiple languages and show the effectiveness of cross-lingual pretraining. We propose two methods to learn cross-lingual language models (XLMs): one unsu- pervised that only relies on monolingual data, and one supervised that leverages parallel data with a new cross-lingual language model objective. We obtain state-of- the-art results on cross-lingual classiﬁcation, unsupervised and supervised machine translation. On XNLI, our approach pushes the state of the art by an absolute gain of 4.9% accuracy. On unsupervised machine translation, we obtain 34.3 BLEU on WMT’16 German-English, improving the previous state of the art by more than 9 BLEU. On supervised machine translation, we obtain a new state of the art of 38.5 BLEU on WMT’16 Romanian-English, outperforming the previous best approach by more than 4 BLEU. Our code and pretrained models are publicly available1.", "values": ["novelty", "generalization", "quantitative evidence (e.g. experiments)", "preciseness", "performance", "efficiency", "effectiveness", "improvement", "requires few resources", "facilitating use (e.g. sharing code)", "not socially biased", "building on past work"], "labels_source": "Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "ed7c7c079c8c54d3b82e016cc52a7a2c3a61f237", "title": "Efficient projections onto the l1-ball for learning in high dimensions", "authors": ["John Duchi", "Shai Shalev-Shwartz", "Yoram Singer", "Tushar Chandra"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "1390156.1390191.pdf", "abstract": "We describe efﬁcient algorithms for projecting a vector onto the ℓ1-ball. We present two methods for projection. The ﬁrst performs exact projec- tion in O(n) expected time, where n is the di- mension of the space. The second works on vec- tors k of whose elements are perturbed outside the ℓ1-ball, projecting in O(k log(n)) time. This setting is especially useful for online learning in sparse feature spaces such as text categorization applications. We demonstrate the merits and ef- fectiveness of our algorithms in numerous batch and online learning tasks. We show that vari- ants of stochastic gradient projection methods augmented with our efﬁcient projection proce- dures outperform interior point methods, which are considered state-of-the-art optimization tech- niques. We also show that in online settings gra- dient updates with ℓ1 projections outperform the exponentiated gradient algorithm while obtain- ing models with high degrees of sparsity.", "values": ["novelty", "generalization", "formal description/analysis", "theoretical guarantees", "approximation", "quantitative evidence (e.g. experiments)", "qualitative evidence (e.g. examples)", "scientific methodology", "large scale", "generality", "exactness", "performance", "efficiency", "effectiveness", "unifying ideas or integrating components", "improvement", "used in practice/popular", "easy to implement", "useful", "interpretable (to users)", "easy to work with", "building on past work"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "f53936c03fb089cc159c551081124aae8a32ec1a", "title": "Isolating Sources of Disentanglement in Variational Autoencoders", "authors": ["Tian Qi Chen", "Xuechen Li", "Roger B. Grosse", "David Duvenaud"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "NeurIPS-2018-isolating-sources-of-disentanglement-in-variational-autoencoders-Paper.pdf", "abstract": "We decompose the evidence lower bound to show the existence of a term measuring the total correlation between latent variables. We use this to motivate the β-TCVAE (Total Correlation Variational Autoencoder) algorithm, a reﬁnement and plug-in replacement of the β-VAE for learning disentangled representations, requiring no additional hyperparameters during training. We further propose a principled classiﬁer-free measure of disentanglement called the mutual information gap (MIG). We perform extensive quantitative and qualitative experiments, in both restricted and non-restricted settings, and show a strong relation between total correlation and disentanglement, when the model is trained using our framework.", "values": ["novelty", "simplicity", "generalization", "robustness", "theoretical guarantees", "quantitative evidence (e.g. experiments)", "qualitative evidence (e.g. examples)", "scientific methodology", "generality", "principled", "automatic", "performance", "efficiency", "successful", "unifying ideas or integrating components", "understanding (for researchers)", "easy to implement", "requires few resources", "parallelizability / distributed", "facilitating use (e.g. sharing code)", "useful", "interpretable (to users)", "beneficence", "explicability", "security", "building on past work"], "labels_source": "Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "f7f73185e3975bb62a3c42b2ba6bd4db57fee8ed", "title": "Certified Adversarial Robustness via Randomized Smoothing", "authors": ["Jeremy M. Cohen", "Elan Rosenfeld", "Jeremy Zico Kolter"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "Copy of cohen19c.pdf", "abstract": "We show how to turn any classiﬁer that classiﬁes well under Gaussian noise into a new classiﬁer that is certiﬁably robust to adversarial perturba- tions under the ℓ2 norm. While this “randomized smoothing” technique has been proposed before in the literature, we are the ﬁrst to provide a tight analysis, which establishes a close connection between ℓ2 robustness and Gaussian noise. We use the technique to train an ImageNet classiﬁer with e.g. a certiﬁed top-1 accuracy of 49% un- der adversarial perturbations with ℓ2 norm less than 0.5 (=127/255). Smoothing is the only ap- proach to certiﬁably robust classiﬁcation which has been shown feasible on full-resolution Im- ageNet. On smaller-scale datasets where com- peting approaches to certiﬁed ℓ2 robustness are viable, smoothing delivers higher certiﬁed accura- cies. The empirical success of the approach sug- gests that provable methods based on randomiza- tion at prediction time are a promising direction for future research into adversarially robust classi- ﬁcation. Code and models are available at http: //github.com/locuslab/smoothing.", "values": ["simplicity", "generalization", "robustness", "formal description/analysis", "theoretical guarantees", "quantitative evidence (e.g. experiments)", "preciseness", "performance", "identifying limitations", "easy to implement", "facilitating use (e.g. sharing code)", "scales up", "applies to real world", "security", "building on past work"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
{"paper_id": "fe9b8aac9fa3bfd9724db5a881a578e471e612d7", "title": "Efficient Neural Architecture Search via Parameter Sharing", "authors": ["Hieu Pham", "Hieu Pham", "Melody Y. Guan", "Barret Zoph", "Quoc V. Le", "Jeff Dean"], "year": null, "venue": null, "doi": null, "url": null, "arxiv_id": null, "pdf_filename": "Copy of pham18a.pdf", "abstract": "We propose Efﬁcient Neural Architecture Search (ENAS), a fast and inexpensive approach for au- tomatic model design. ENAS constructs a large computational graph, where each subgraph repre- sents a neural network architecture, hence forcing all architectures to share their parameters. A con- troller is trained with policy gradient to search for a subgraph that maximizes the expected reward on a validation set. Meanwhile a model correspond- ing to the selected subgraph is trained to minimize a canonical cross entropy loss. Sharing parame- ters among child models allows ENAS to deliver strong empirical performances, whilst using much fewer GPU-hours than existing automatic model design approaches, and notably, 1000x less ex- pensive than standard Neural Architecture Search. On Penn Treebank, ENAS discovers a novel ar- chitecture that achieves a test perplexity of 56.3, on par with the existing state-of-the-art among all methods without post-training processing. On CIFAR-10, ENAS ﬁnds a novel architecture that achieves 2.89% test error, which is on par with the 2.65% test error of NASNet (Zoph et al., 2018).", "values": ["novelty", "generalization", "quantitative evidence (e.g. experiments)", "large scale", "promising", "performance", "efficiency", "successful", "identifying limitations", "improvement", "requires few resources", "building on past work"], "labels_source": "No Moral Values", "abstract_extraction": {"used_ocr": false, "pages_checked": 2}}
