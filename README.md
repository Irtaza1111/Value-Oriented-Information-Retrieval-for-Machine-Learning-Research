# Value-Oriented-Information-Retrieval-for-Machine-Learning-Research
In this project, we study value-oriented information retrieval for ML research by comparing a sparse BM25 baseline with a dense retrieval approach based on Sentence-BERT embeddings. Using abstracts from ML papers annotated with moral values by Birhane et al. (2022),
Machine learning (ML) research increasingly influences society, motivating concerns about the values implicitly encoded in technical work. Prior studies have shown that ML publications often reflect moral and normative values such as fairness, robustness, efficiency, and social benefit, even when these values are not explicitly stated. Birhane et al. (2022) Conducted one of the first systematic studies on moral values in AI research.
She Manually annotated 99 Most cited NeurIPS and ICML papers for moral content(2008, 2009, 2018, 2019)  provided a large-scale annotation of ML research papers with value-based labels, offering a unique opportunity to study value-oriented retrieval in an information retrieval (IR) setting.
Traditional IR systems primarily rely on lexical overlap between queries and documents, which may be insufficient for retrieving papers that express moral or ethical values implicitly. Dense retrieval methods based on transformer models promise improved semantic understanding and may better capture such implicit signals. However, it remains unclear to what extent dense retrieval improves value-based retrieval over strong lexical baselines.
In this project, we study value-oriented information retrieval for ML research by comparing a sparse BM25 baseline with a dense retrieval approach based on Sentence-BERT embeddings. Using abstracts from ML papers annotated with moral values by Birhane et al. (2022), we evaluate retrieval performance across a set of value-driven queries. Our goal is to assess whether dense retrieval improves early ranking performance for value-specific queries, particularly for low-frequency and ethical concepts such as fairness and beneficence.
We base our experiment on  subset of ML research paper annotated by Birhane et al. (2022). From the available dataset, we automatically matched 94 papers to their corresponding PDFs using title-based matching and extracted their abstracts using a combination of text extraction and OCR. Abstracts were chosen as the document representation to reflect realistic search scenarios and to reduce computational complexity.
Each paper is associated with one or more value labels, such as fairness, robustness, efficiency, generalization, and beneficence. These labels serve as ground truth for relevance judgments. A document is considered relevant to a query if its value labels overlap with the target values associated with that query.
We define a query set consisting of eight value-oriented queries. Each query is mapped to one or more target value labels. The query set includes both narrowly defined ethical values (e.g., fairness, privacy, beneficence) and broadly applicable technical values (e.g., efficiency, generalization). This distinction allows us to analyze retrieval behavior across different types of values.
As a baseline, we implement a BM25 retrieval model over the abstract corpus. Abstracts are tokenized using a simple regex-based tokenizer, and queries are matched against document tokens using the BM25 scoring function. BM25 represents a strong lexical baseline commonly used in IR tasks and provides a reference point for evaluating more advanced methods.
For dense retrieval, we encode all abstracts using a pre-trained Sentence-BERT model (all-MiniLM-L6-v2). This model produces fixed-size dense embeddings that capture semantic similarity beyond surface-level word overlap. All embeddings are normalized and indexed using FAISS with inner-product similarity, enabling efficient nearest-neighbor search.
At query time, queries are encoded using the same Sentence-BERT model and compared against the indexed document embeddings. The top-ranked documents are returned based on cosine similarity.
